{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VJnnwIPnFcLh"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers pandas scikit-learn tqdm textstat spacy accelerate xgboost\n",
        "\n",
        "# Install and download all at once\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "T1jHvWBLGC4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block A — Setup + Load datasets (Drive in, RUN out)\n",
        "# ============================================\n",
        "\n",
        "# (optional) mount Drive first in Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import os, re, math, random, json, contextlib\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Torch / HF\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.use_deterministic_algorithms(False)\n",
        "try:\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Quiet AMP deprecation warnings with thin wrappers\n",
        "from torch.amp import autocast as _autocast, GradScaler as _GradScaler\n",
        "def autocast(enabled=True):\n",
        "    return _autocast(device_type=\"cuda\") if (enabled and torch.cuda.is_available() and enabled) \\\n",
        "           else contextlib.nullcontext()\n",
        "def GradScaler(**kw):\n",
        "    if torch.cuda.is_available():\n",
        "        return _GradScaler(device=\"cuda\", **kw)\n",
        "    return _GradScaler(**kw)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def seed_everything(seed: int = 13):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed);\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# -----------------------------\n",
        "# Config: READ from Drive; SAVE to /content/emc_run\n",
        "# -----------------------------\n",
        "class Config:\n",
        "    # READ inputs/artifacts FROM Drive (put your CSVs here)\n",
        "    DATA_DIR = \"/content/drive/MyDrive/emc\"\n",
        "\n",
        "    # SAVE all new artifacts/checkpoints TO local runtime (won't sync to Drive)\n",
        "    RUN_DIR  = \"/content/emc_run\"\n",
        "\n",
        "    # Data (read from Drive)\n",
        "    TRAIN_PATH     = os.path.join(DATA_DIR, \"train.csv\")\n",
        "    OOD_DATA_PATH  = os.path.join(DATA_DIR, \"ood_dataset.csv\")\n",
        "    TERMS_PATH     = os.path.join(DATA_DIR, \"engineering_terms.csv\")  # used later by FeatureExtractor\n",
        "\n",
        "    # Artifacts (save to /content/emc_run)\n",
        "    XLM_R_MODEL_PATH   = os.path.join(RUN_DIR, \"xlmr_only_outputs/pytorch_model.bin\")\n",
        "    SIMPLE_PT_PATH     = os.path.join(RUN_DIR, \"simple_fusion_outputs/fusion_simple.pt\")\n",
        "    GATED_PT_PATH      = os.path.join(RUN_DIR, \"gated_fusion_outputs/fusion_gated.pt\")\n",
        "    SIMPLE_SCALER_PATH = os.path.join(RUN_DIR, \"simple_fusion_outputs/scaler12.pkl\")\n",
        "    GATED_SCALER_PATH  = os.path.join(RUN_DIR, \"gated_fusion_outputs/scaler12.pkl\")\n",
        "    RESULTS_CSV_PATH   = os.path.join(RUN_DIR, \"domain_adaptation_results.csv\")\n",
        "    DAPT_SAVE_DIR      = os.path.join(RUN_DIR, \"dapt_mlm\")\n",
        "\n",
        "    # Model settings (will be reused later)\n",
        "    MODEL_NAME = \"xlm-roberta-base\"\n",
        "    MAX_LEN = 256\n",
        "    FEAT_DIM = 12\n",
        "    NUM_LABELS = 2  # will be overwritten after we infer\n",
        "\n",
        "    # Training/eval defaults (used later)\n",
        "    VAL_SPLIT = 0.1\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 2\n",
        "    PIN_MEMORY = (DEVICE == \"cuda\")\n",
        "\n",
        "    # Few-shot DA\n",
        "    DA_ADAPT_RATIO = 0.4\n",
        "    DA_EPOCHS = 4\n",
        "    DA_LR = 1.5e-5\n",
        "    DA_UNFREEZE_TOP_N = 16\n",
        "\n",
        "# Make sure local run dirs exist\n",
        "os.makedirs(Config.RUN_DIR, exist_ok=True)\n",
        "for p in [\n",
        "    os.path.dirname(Config.XLM_R_MODEL_PATH),\n",
        "    os.path.dirname(Config.SIMPLE_PT_PATH),\n",
        "    os.path.dirname(Config.GATED_PT_PATH),\n",
        "    os.path.dirname(Config.SIMPLE_SCALER_PATH),\n",
        "    os.path.dirname(Config.GATED_SCALER_PATH),\n",
        "    Config.DAPT_SAVE_DIR,\n",
        "]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Robust data loading helpers\n",
        "# -----------------------------\n",
        "def _require_file(path: str, name: str):\n",
        "    if not os.path.isfile(path):\n",
        "        raise FileNotFoundError(f\"Missing {name} at: {path}\")\n",
        "\n",
        "def _clean_text_series(s: pd.Series) -> pd.Series:\n",
        "    # ensure str; strip; replace NaN with \"\"\n",
        "    s = s.astype(str).fillna(\"\").map(lambda x: x.strip())\n",
        "    # collapse super long whitespace\n",
        "    s = s.map(lambda x: re.sub(r\"\\s+\", \" \", x))\n",
        "    return s\n",
        "\n",
        "def _ensure_lang_col(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if 'lang' not in df.columns:\n",
        "        df = df.copy()\n",
        "        df['lang'] = 'en'\n",
        "    return df\n",
        "\n",
        "def _drop_empty_rows(df: pd.DataFrame, text_col=\"content\") -> pd.DataFrame:\n",
        "    before = len(df)\n",
        "    df = df[df[text_col].astype(str).str.strip().str.len() > 0].copy()\n",
        "    after = len(df)\n",
        "    if after < before:\n",
        "        print(f\"• Dropped {before - after} empty-text rows\")\n",
        "    return df\n",
        "\n",
        "def build_global_label_mapping(train_df: pd.DataFrame, ood_df: pd.DataFrame) -> Dict[Any, int]:\n",
        "    \"\"\"\n",
        "    Build a consistent mapping over ALL labels in train + OOD so that\n",
        "    labels are contiguous ints 0..K-1. Return dict original_label -> idx.\n",
        "    \"\"\"\n",
        "    all_labels = pd.concat([train_df['label'], ood_df['label']], axis=0)\n",
        "    # If already int and contiguous 0..K-1, keep identity mapping\n",
        "    if pd.api.types.is_integer_dtype(all_labels):\n",
        "        uniq = sorted(all_labels.unique().tolist())\n",
        "        if uniq and uniq[0] == 0 and uniq[-1] == len(uniq)-1:\n",
        "            return {i: i for i in uniq}\n",
        "    # Otherwise, map sorted unique values to 0..K-1\n",
        "    uniq = sorted(all_labels.unique().tolist(), key=lambda x: str(x))\n",
        "    return {v: i for i, v in enumerate(uniq)}\n",
        "\n",
        "def apply_label_mapping(df: pd.DataFrame, mapping: Dict[Any, int]) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df['label'] = df['label'].map(mapping)\n",
        "    if df['label'].isna().any():\n",
        "        missing = df[df['label'].isna()]\n",
        "        raise ValueError(f\"Found labels not in mapping: {missing['label'].tolist()[:5]} ...\")\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    return df\n",
        "\n",
        "def summarize_dataset(train_df: pd.DataFrame, ood_df: pd.DataFrame):\n",
        "    print(\"\\n=== Dataset summary ===\")\n",
        "    print(f\"device: {DEVICE}\")\n",
        "    print(f\"train: {len(train_df):,} rows | ood: {len(ood_df):,} rows\")\n",
        "    # train label dist\n",
        "    tl = train_df['label'].value_counts().sort_index()\n",
        "    print(\"train label distribution:\", tl.to_dict())\n",
        "    if 'domain' in ood_df.columns:\n",
        "        dom_counts = ood_df['domain'].value_counts().to_dict()\n",
        "        print(\"ood domains:\", dom_counts)\n",
        "    print(\"=======================\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load, clean, map labels\n",
        "# -----------------------------\n",
        "seed_everything(13)\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# require CSVs on Drive\n",
        "_require_file(Config.TRAIN_PATH, \"train.csv\")\n",
        "_require_file(Config.OOD_DATA_PATH, \"ood_dataset.csv\")\n",
        "_require_file(Config.TERMS_PATH, \"engineering_terms.csv\")  # used later; just verifying now\n",
        "\n",
        "# read\n",
        "train_df = pd.read_csv(Config.TRAIN_PATH)\n",
        "ood_df   = pd.read_csv(Config.OOD_DATA_PATH)\n",
        "\n",
        "# basic column checks\n",
        "for c in ['content', 'label']:\n",
        "    if c not in train_df.columns:\n",
        "        raise KeyError(f\"train.csv missing required column: {c}\")\n",
        "for c in ['content', 'label', 'domain']:\n",
        "    if c not in ood_df.columns:\n",
        "        raise KeyError(f\"ood_dataset.csv missing required column: {c}\")\n",
        "\n",
        "# clean text; ensure lang; drop empty rows\n",
        "train_df = train_df.copy()\n",
        "train_df['content'] = _clean_text_series(train_df['content'])\n",
        "train_df = _ensure_lang_col(train_df)\n",
        "train_df = _drop_empty_rows(train_df, 'content')\n",
        "\n",
        "ood_df = ood_df.copy()\n",
        "ood_df['content'] = _clean_text_series(ood_df['content'])\n",
        "ood_df = _ensure_lang_col(ood_df)\n",
        "ood_df = _drop_empty_rows(ood_df, 'content')\n",
        "\n",
        "# build & apply a GLOBAL label mapping  (IMPORTANT for consistency)\n",
        "label_map = build_global_label_mapping(train_df, ood_df)\n",
        "with open(os.path.join(Config.RUN_DIR, \"label_mapping.json\"), \"w\") as f:\n",
        "    json.dump({str(k): int(v) for k, v in label_map.items()}, f, indent=2)\n",
        "\n",
        "train_df = apply_label_mapping(train_df, label_map)\n",
        "ood_df   = apply_label_mapping(ood_df,   label_map)\n",
        "\n",
        "# infer NUM_LABELS from mapped data and store into Config\n",
        "Config.NUM_LABELS = int(pd.unique(pd.concat([train_df['label'], ood_df['label']], axis=0)).shape[0])\n",
        "print(\"Detected NUM_LABELS =\", Config.NUM_LABELS)\n",
        "\n",
        "# (optional) persist cleaned copies used for all runs (for reproducibility)\n",
        "train_clean_path = os.path.join(Config.RUN_DIR, \"train_clean.csv\")\n",
        "ood_clean_path   = os.path.join(Config.RUN_DIR, \"ood_clean.csv\")\n",
        "train_df.to_csv(train_clean_path, index=False)\n",
        "ood_df.to_csv(ood_clean_path, index=False)\n",
        "print(f\"Saved cleaned datasets to:\\n  {train_clean_path}\\n  {ood_clean_path}\")\n",
        "\n",
        "# quick summary\n",
        "summarize_dataset(train_df, ood_df)\n",
        "\n",
        "# NOTE:\n",
        "# - Next cells (training, DAPT loading, fusion, adaptation, etc.) should\n",
        "#   reference paths from Config.* and use the already-loaded train_df/ood_df.\n",
        "# - All artifacts will be saved to /content/emc_run (RUN_DIR), keeping Drive clean.\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FWz1DBgspEDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block B — Tokenizer, features, datasets, fusion models & utils\n",
        "# ============================================\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "\n",
        "# --- Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
        "\n",
        "# --- Terms lexicon & 12D feature extractor -----------------\n",
        "import re\n",
        "\n",
        "_WORD_RE = re.compile(r\"\\w+\", re.UNICODE)\n",
        "def simple_words(t: str): return _WORD_RE.findall(t or \"\")\n",
        "\n",
        "def sent_count(t: str) -> int:\n",
        "    if not t: return 0\n",
        "    return max(1, len(re.split(r'[.!?]+[\\s\\n]+', t)))\n",
        "\n",
        "def punct_count(t: str) -> int:\n",
        "    return sum(1 for ch in (t or \"\") if ch in \".,;:!?\")\n",
        "\n",
        "class TermsLexicon:\n",
        "    def __init__(self, csv_path: str, term_col=\"terms\", lang_col=\"lang\"):\n",
        "        import pandas as pd, os\n",
        "        if not os.path.exists(csv_path): raise FileNotFoundError(csv_path)\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if term_col not in df.columns: raise ValueError(f\"Missing '{term_col}'\")\n",
        "        if lang_col not in df.columns: df[lang_col] = 'en'\n",
        "        self.by_lang = {\n",
        "            str(l).lower(): set(\n",
        "                str(x).strip().lower()\n",
        "                for x in d[term_col].dropna().tolist() if str(x).strip()\n",
        "            )\n",
        "            for l, d in df.groupby(lang_col)\n",
        "        }\n",
        "\n",
        "    def pct_in_text(self, text: str, lang: str) -> float:\n",
        "        if not text: return 0.0\n",
        "        terms = self.by_lang.get((lang or \"en\").lower(), set())\n",
        "        if not terms: return 0.0\n",
        "        ws = [w.lower() for w in simple_words(text)]\n",
        "        if not ws: return 0.0\n",
        "        return sum(1 for w in ws if w in terms) / max(1, len(ws))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "_NUM_RE = re.compile(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\")\n",
        "def extract_numbers(text: str):\n",
        "    nums, dec = [], 0\n",
        "    for m in _NUM_RE.finditer(text or \"\"):\n",
        "        s = m.group(0)\n",
        "        try:\n",
        "            v = float(s)\n",
        "            if ('.' in s) or ('e' in s.lower()): dec += 1\n",
        "            nums.append(abs(v))\n",
        "        except: pass\n",
        "    return nums, dec\n",
        "\n",
        "def _finite_or_zero(x: float) -> float:\n",
        "    return float(x) if np.isfinite(x) else 0.0\n",
        "\n",
        "# Optional textstat for English readability\n",
        "try:\n",
        "    import textstat\n",
        "    _HAS_TEXTSTAT = True\n",
        "except Exception:\n",
        "    _HAS_TEXTSTAT = False\n",
        "\n",
        "STD_TERMS = {\"iso\",\"asme\",\"ieee\",\"din\",\"ansi\",\"iec\",\"ul\",\"astm\",\"en\"}\n",
        "SAFETY_TERMS = {\"safety\",\"hazard\",\"warning\",\"risk\",\"caution\",\"danger\",\"emergency\"}\n",
        "\n",
        "class FeatureExtractor12:\n",
        "    MAX_CHARS, MAX_WORDS, MAX_SENTS, MAX_PUNCT, MAX_NUM_COUNT = 200_000, 40_000, 10_000, 50_000, 10_000\n",
        "    MAX_NUM_ABS, MAX_AVG_MAG = 1e12, 1e12\n",
        "    def __init__(self, tlex: TermsLexicon): self.tlex = tlex\n",
        "\n",
        "    def extract_one(self, text: str, lang: str) -> np.ndarray:\n",
        "        text = \"\" if text is None else str(text); lang = (lang or \"en\").lower()\n",
        "        ws = simple_words(text); n_words = len(ws)\n",
        "        chars = min(len(text), self.MAX_CHARS)\n",
        "        words = min(n_words, self.MAX_WORDS)\n",
        "        sents = min(sent_count(text), self.MAX_SENTS)\n",
        "\n",
        "        if lang == \"en\" and _HAS_TEXTSTAT and text.strip():\n",
        "            fre = _finite_or_zero(textstat.flesch_reading_ease(text))\n",
        "            fog = _finite_or_zero(textstat.gunning_fog(text))\n",
        "        else:\n",
        "            fre = fog = 0.0\n",
        "\n",
        "        eng_pct = self.tlex.pct_in_text(text, lang)\n",
        "        punc = min(punct_count(text), self.MAX_PUNCT)\n",
        "\n",
        "        nums, dec_cnt = extract_numbers(text)\n",
        "        nnums = min(len(nums), self.MAX_NUM_COUNT)\n",
        "        avg_mag = min(float(np.mean([min(v, self.MAX_NUM_ABS) for v in nums])) if nums else 0.0, self.MAX_AVG_MAG)\n",
        "        dec_ratio = float(dec_cnt / len(nums)) if nums else 0.0\n",
        "\n",
        "        low = text.lower()\n",
        "        has_std = 1.0 if any(t in low for t in STD_TERMS) else 0.0\n",
        "        has_saf = 1.0 if any(t in low for t in SAFETY_TERMS) else 0.0\n",
        "\n",
        "        feats = np.array([chars, words, sents, fre, fog, eng_pct, punc, nnums, has_std, has_saf, avg_mag, dec_ratio], dtype=np.float32)\n",
        "        if not np.all(np.isfinite(feats)):\n",
        "            feats = np.nan_to_num(feats, nan=0.0, posinf=self.MAX_AVG_MAG, neginf=0.0)\n",
        "        return feats.astype(np.float32)\n",
        "\n",
        "    def extract_df(self, df: pd.DataFrame) -> np.ndarray:\n",
        "        assert 'content' in df.columns\n",
        "        if 'lang' not in df.columns:\n",
        "            df = df.copy(); df['lang'] = 'en'\n",
        "        rows = [self.extract_one(r.get(\"content\",\"\"), r.get(\"lang\",\"en\")) for _, r in tqdm(df.iterrows(), total=len(df), desc=\"Extracting features\")]\n",
        "        return np.stack(rows, axis=0).astype(np.float32)\n",
        "\n",
        "tlex = TermsLexicon(Config.TERMS_PATH)\n",
        "fe = FeatureExtractor12(tlex)\n",
        "\n",
        "# --- Datasets ----------------------------------------------\n",
        "class TextFeatDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, tokenizer, feats: Optional[np.ndarray] = None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tok = tokenizer\n",
        "        self.feats = feats\n",
        "        self.labels = df['label'].values.astype(int) if 'label' in df.columns else None\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        enc = self.tok(\n",
        "            str(row['content']),\n",
        "            truncation=True,\n",
        "            max_length=Config.MAX_LEN,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {\n",
        "            'input_ids': enc['input_ids'].squeeze(0),\n",
        "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
        "        }\n",
        "        if self.feats is not None:\n",
        "            item['feats'] = torch.tensor(self.feats[idx], dtype=torch.float32)\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "def make_loader(dataset, batch_size, sampler=None, shuffle=False):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=sampler,\n",
        "        shuffle=(shuffle if sampler is None else False),\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=Config.PIN_MEMORY,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "# --- Fusion models ------------------------------------------\n",
        "def masked_mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "    mask = attention_mask.unsqueeze(-1).float()\n",
        "    return (last_hidden_state * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n",
        "\n",
        "class SimpleFusion(nn.Module):\n",
        "    def __init__(self, model_name: str, n_feats: int, n_labels: int = None):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        try: self.encoder.gradient_checkpointing_enable()\n",
        "        except Exception: pass\n",
        "        H = self.encoder.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(H + n_feats, n_labels or Config.NUM_LABELS)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, feats):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        pooled = masked_mean_pool(out.last_hidden_state, attention_mask)\n",
        "        fused = torch.cat([pooled, feats], dim=1)\n",
        "        return self.classifier(self.dropout(fused))\n",
        "\n",
        "class GatedFusion(nn.Module):\n",
        "    def __init__(self, model_name: str, n_feats: int, n_labels: int = None, feat_proj: int = 64):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        try: self.encoder.gradient_checkpointing_enable()\n",
        "        except Exception: pass\n",
        "        H = self.encoder.config.hidden_size\n",
        "        self.fe_proj = nn.Sequential(nn.Linear(n_feats, feat_proj), nn.ReLU())\n",
        "        self.gate = nn.Sequential(nn.Linear(H + n_feats, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(H + feat_proj, n_labels or Config.NUM_LABELS)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, feats):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        pooled = masked_mean_pool(out.last_hidden_state, attention_mask)\n",
        "        alpha = self.gate(torch.cat([pooled, feats], dim=1))\n",
        "        ef = self.fe_proj(feats)\n",
        "        fused = torch.cat([pooled, alpha * ef], dim=1)\n",
        "        return self.classifier(self.dropout(fused))\n",
        "\n",
        "def unfreeze_top_n(model: nn.Module, top_n: int):\n",
        "    # Works for HF encoders (roberta/xlm_roberta)\n",
        "    enc = (getattr(model, \"base_model\", None)\n",
        "           or getattr(model, \"roberta\", None)\n",
        "           or getattr(model, \"xlm_roberta\", None)\n",
        "           or getattr(model, \"encoder\", None)  # for fusion models' encoder\n",
        "          )\n",
        "    if enc is None:\n",
        "        return\n",
        "    if hasattr(enc, \"encoder\") and hasattr(enc.encoder, \"layer\"):\n",
        "        layers = enc.encoder.layer\n",
        "        K = len(layers)\n",
        "        for i, layer in enumerate(layers):\n",
        "            req = (i >= K - top_n)\n",
        "            for p in layer.parameters(): p.requires_grad = req\n",
        "    # always keep classifier trainable\n",
        "    if hasattr(model, \"classifier\"):\n",
        "        for p in model.classifier.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "print(\"Block B ready ✔️\")\n"
      ],
      "metadata": {
        "id": "NofsAp7Bpgyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block C — DAPT training + DAPT/Head loaders (robust copy)\n",
        "# ============================================\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForMaskedLM, AutoModelForSequenceClassification,\n",
        "    DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        ")\n",
        "\n",
        "# --- Unlabeled dataset for MLM\n",
        "class UnlabeledTextDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], tokenizer, max_len: int):\n",
        "        self.texts = texts\n",
        "        self.tok = tokenizer\n",
        "        self.max_len = max_len\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tok(\n",
        "            str(self.texts[idx]),\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {'input_ids': enc['input_ids'].squeeze(0), 'attention_mask': enc['attention_mask'].squeeze(0)}\n",
        "\n",
        "def run_mlm_corpus_training(\n",
        "    texts: List[str],\n",
        "    save_dir: str,\n",
        "    epochs: int,\n",
        "    batch: int,\n",
        "    lr: float,\n",
        "    warmup_ratio: float,\n",
        "    max_steps: Optional[int],\n",
        "    tokenizer,\n",
        "):\n",
        "    if len(texts) == 0:\n",
        "        print(\"⚠️ DAPT skipped: no texts\")\n",
        "        return\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model = AutoModelForMaskedLM.from_pretrained(Config.MODEL_NAME)\n",
        "    model.to(DEVICE)\n",
        "    # ✅ use shorter seq length for MLM to be memory-safe\n",
        "    dataset = UnlabeledTextDataset(texts, tokenizer, max_len=Config.MLM_MAX_LEN)\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=save_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch,\n",
        "        learning_rate=lr,\n",
        "        warmup_ratio=warmup_ratio,\n",
        "        logging_steps=50,\n",
        "        save_strategy=\"no\",   # keep only final weights we save manually\n",
        "        fp16=(DEVICE==\"cuda\"),\n",
        "        gradient_checkpointing=True,\n",
        "        max_steps=max_steps,\n",
        "        dataloader_num_workers=Config.NUM_WORKERS,\n",
        "        report_to=\"none\",\n",
        "        disable_tqdm=True,\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    print(f\"Starting DAPT MLM on {DEVICE} …\")\n",
        "    trainer.train()\n",
        "    # Write a plain state_dict() to a simple bin path\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
        "    print(\"DAPT complete.\")\n",
        "\n",
        "# ---- Helper: materialize the MLM model then copy its encoder 1:1\n",
        "def _materialize_mlm_encoder_state(bin_path: str, base_model_name: str):\n",
        "    \"\"\"\n",
        "    Loads a base MLM model, applies the saved state_dict (whatever its prefixes),\n",
        "    and returns *its encoder* state_dict. This sidesteps key-prefix mismatches.\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(bin_path):\n",
        "        return None\n",
        "    # Create a fresh MLM model with the same architecture\n",
        "    mlm = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
        "    sd = torch.load(bin_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"state_dict\" in sd:\n",
        "        sd = sd[\"state_dict\"]\n",
        "    # Strip a leading \"model.\" if present\n",
        "    if sd and all(k.startswith(\"model.\") for k in sd.keys()):\n",
        "        sd = {k[len(\"model.\"):]: v for k, v in sd.items()}\n",
        "    # Load what we can (strict=False is intentional)\n",
        "    mlm.load_state_dict(sd, strict=False)\n",
        "\n",
        "    src_enc = (getattr(mlm, \"roberta\", None)\n",
        "               or getattr(mlm, \"xlm_roberta\", None))\n",
        "    if src_enc is None:\n",
        "        return None\n",
        "    return src_enc.state_dict()\n",
        "\n",
        "def safe_load_seqcls_from_dapt(dapt_dir: str, num_labels: int, base_model_name: str):\n",
        "    \"\"\"\n",
        "    Build a SequenceClassification model and copy ONLY encoder weights from\n",
        "    the DAPT MLM checkpoint by *materializing the MLM encoder* first.\n",
        "    \"\"\"\n",
        "    m = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=num_labels)\n",
        "    try: m.gradient_checkpointing_enable()\n",
        "    except Exception: pass\n",
        "\n",
        "    bin_path = os.path.join(dapt_dir, \"pytorch_model.bin\")\n",
        "    src_enc_sd = _materialize_mlm_encoder_state(bin_path, base_model_name)\n",
        "    if src_enc_sd is None:\n",
        "        print(f\"⚠️ No usable DAPT bin at {bin_path}; using base encoder.\")\n",
        "        return m\n",
        "\n",
        "    dest_enc = (getattr(m, \"base_model\", None)\n",
        "                or getattr(m, \"roberta\", None)\n",
        "                or getattr(m, \"xlm_roberta\", None))\n",
        "    if dest_enc is None:\n",
        "        print(\"⚠️ Could not locate encoder in classifier; skipping DAPT copy.\")\n",
        "        return m\n",
        "\n",
        "    dest_sd = dest_enc.state_dict()\n",
        "    copied = 0\n",
        "    new_sd = {}\n",
        "    for k in dest_sd.keys():\n",
        "        if k in src_enc_sd:\n",
        "            new_sd[k] = src_enc_sd[k]\n",
        "            copied += 1\n",
        "    dest_enc.load_state_dict({**dest_sd, **new_sd}, strict=False)\n",
        "    print(f\"✅ DAPT encoder load (robust): copied {copied}/{len(dest_sd)} tensors from {bin_path}\")\n",
        "    return m\n",
        "\n",
        "def load_ft_classifier_only_into(model, ft_state_path: str):\n",
        "    \"\"\"Load ONLY the classifier head from an in-domain fine-tuned checkpoint.\"\"\"\n",
        "    if not os.path.isfile(ft_state_path):\n",
        "        print(f\"⚠️ Classifier ckpt not found at {ft_state_path}\")\n",
        "        return\n",
        "    sd = torch.load(ft_state_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"state_dict\" in sd:\n",
        "        sd = sd[\"state_dict\"]\n",
        "    head = {k: v for k, v in sd.items() if k.startswith(\"classifier.\")}\n",
        "    missing, unexpected = model.load_state_dict(head, strict=False)\n",
        "    print(f\"✅ Loaded classifier head from {ft_state_path} (kept encoder). \"\n",
        "          f\"missing={len(missing)} unexpected={len(unexpected)}\")\n",
        "\n",
        "def fusion_available(pt_path: str, scaler_path: str) -> bool:\n",
        "    return os.path.isfile(pt_path) and os.path.isfile(scaler_path)\n",
        "\n",
        "def load_dapt_into_fusion_encoder(fusion_model, dapt_dir: str, base_model_name: str):\n",
        "    \"\"\"Copy DAPT encoder weights into fusion.encoder using the robust source encoder trick.\"\"\"\n",
        "    bin_path = os.path.join(dapt_dir, \"pytorch_model.bin\")\n",
        "    src_enc_sd = _materialize_mlm_encoder_state(bin_path, base_model_name)\n",
        "    if src_enc_sd is None:\n",
        "        print(f\"⚠️ No usable DAPT bin at {bin_path}; skipping DAPT→fusion.\")\n",
        "        return\n",
        "    enc = getattr(fusion_model, \"encoder\", None)\n",
        "    if enc is None:\n",
        "        print(\"⚠️ Fusion has no .encoder; skip.\")\n",
        "        return\n",
        "    dest_sd = enc.state_dict()\n",
        "    copied = 0\n",
        "    new_sd = {}\n",
        "    for k in dest_sd.keys():\n",
        "        if k in src_enc_sd:\n",
        "            new_sd[k] = src_enc_sd[k]\n",
        "            copied += 1\n",
        "    enc.load_state_dict({**dest_sd, **new_sd}, strict=False)\n",
        "    print(f\"✅ DAPT→Fusion encoder (robust): copied {copied}/{len(dest_sd)} tensors from {bin_path}\")\n",
        "\n",
        "print(\"Block C ready ✔️\")\n"
      ],
      "metadata": {
        "id": "o_t6zJBDqJCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block D — Supervised fine-tuning (base classifier)\n",
        "# ============================================\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_transformer_supervised(train_df: pd.DataFrame, tokenizer, num_labels: int):\n",
        "    tr, va = train_test_split(train_df, test_size=Config.VAL_SPLIT, stratify=train_df['label'], random_state=13)\n",
        "    ds_tr = TextFeatDataset(tr, tokenizer, feats=None)\n",
        "    ds_va = TextFeatDataset(va, tokenizer, feats=None)\n",
        "    dl_tr = make_loader(ds_tr, batch_size=32, shuffle=True)\n",
        "    dl_va = make_loader(ds_va, batch_size=64)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(Config.MODEL_NAME, num_labels=num_labels).to(DEVICE)\n",
        "    try: model.gradient_checkpointing_enable()\n",
        "    except Exception: pass\n",
        "\n",
        "    # Freeze embeddings + bottom layers for stability\n",
        "    enc = (getattr(model, \"base_model\", None)\n",
        "           or getattr(model, \"roberta\", None)\n",
        "           or getattr(model, \"xlm_roberta\", None))\n",
        "    if enc is not None and hasattr(enc, \"embeddings\"):\n",
        "        for p in enc.embeddings.parameters(): p.requires_grad = False\n",
        "    if enc is not None and hasattr(enc, \"encoder\") and hasattr(enc.encoder, \"layer\"):\n",
        "        for i, layer in enumerate(enc.encoder.layer):\n",
        "            if i < 4:  # freeze bottom 4\n",
        "                for p in layer.parameters(): p.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, weight_decay=0.01)\n",
        "    total_steps = len(dl_tr) * 3\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, int(0.06*total_steps), total_steps)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    best_f1, bad = -1.0, 0\n",
        "    for epoch in range(3):\n",
        "        model.train(); running = 0.0\n",
        "        for b in tqdm(dl_tr, desc=f\"FT Epoch {epoch+1}/3\", leave=False):\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); y=b['labels'].to(DEVICE)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                out = model(input_ids=ids, attention_mask=am, return_dict=True)\n",
        "                loss = loss_fn(out.logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer); scaler.update(); scheduler.step()\n",
        "            running += float(loss.item())\n",
        "\n",
        "        # val\n",
        "        f1 = evaluate_model(model, dl_va, 'transformer')\n",
        "        print(f\"Epoch {epoch+1}: loss={running/max(1,len(dl_tr)):.4f}  val_macroF1={f1:.4f}\")\n",
        "        if f1 > best_f1:\n",
        "            best_f1, bad = f1, 0\n",
        "            os.makedirs(os.path.dirname(Config.XLM_R_MODEL_PATH), exist_ok=True)\n",
        "            torch.save(model.state_dict(), Config.XLM_R_MODEL_PATH)\n",
        "            print(\"✅ Saved best model to RUN_DIR.\")\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= 2:\n",
        "                print(\"⏹️ Early stopping\")\n",
        "                break\n",
        "\n",
        "print(\"Block D ready ✔️\")\n"
      ],
      "metadata": {
        "id": "Vs4AdEWcqN2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block E — Eval/proba/uncertainty + adaptation\n",
        "# ============================================\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def evaluate_model(model, data_loader, model_type: str = 'transformer') -> float:\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=f\"Evaluating {model_type}\", leave=False):\n",
        "            ids = batch['input_ids'].to(DEVICE); am = batch['attention_mask'].to(DEVICE)\n",
        "            if 'labels' in batch:\n",
        "                all_labels.extend(batch['labels'].cpu().numpy().tolist())\n",
        "\n",
        "            if model_type == 'transformer':\n",
        "                with autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                    logits = model(input_ids=ids, attention_mask=am, return_dict=True).logits\n",
        "            elif model_type in ('simple','gated'):\n",
        "                feats = batch['feats'].to(DEVICE)\n",
        "                logits = model(ids, am, feats)\n",
        "            else:\n",
        "                raise ValueError(model_type)\n",
        "\n",
        "            preds = logits.argmax(1).cpu().tolist()\n",
        "            all_preds.extend(preds)\n",
        "    return f1_score(all_labels, all_preds, average='macro') if all_labels else float(\"nan\")\n",
        "\n",
        "def get_probs_transformer(model, ds, idx, batch=64):\n",
        "    if len(idx) == 0: return np.array([])\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out = []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE)\n",
        "            with autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                logits = model(input_ids=ids, attention_mask=am, return_dict=True).logits\n",
        "            p = torch.softmax(logits, dim=-1)[:, 1].cpu().numpy()\n",
        "            out.append(p)\n",
        "    return np.concatenate(out) if out else np.array([])\n",
        "\n",
        "def get_probs_fusion(model, ds, idx, batch=64):\n",
        "    if len(idx)==0: return np.array([])\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out=[]\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); feats=b['feats'].to(DEVICE)\n",
        "            logits = model(ids, am, feats)\n",
        "            p = torch.softmax(logits, dim=-1)[:, 1].cpu().numpy()\n",
        "            out.append(p)\n",
        "    return np.concatenate(out) if out else np.array([])\n",
        "\n",
        "def pick_uncertain_indices_with_proba(proba_fn, k, n):\n",
        "    idx_all = np.arange(n)\n",
        "    if k <= 0 or n <= k:\n",
        "        return np.array([], dtype=int), idx_all\n",
        "    probs = proba_fn(idx_all)\n",
        "    if probs.size == 0:\n",
        "        rng = np.random.default_rng(13); rng.shuffle(idx_all)\n",
        "        return idx_all[:k], idx_all[k:]\n",
        "    p = np.clip(probs, 1e-6, 1-1e-6)\n",
        "    ent = -(p*np.log(p) + (1-p)*np.log(1-p))\n",
        "    order = np.argsort(-ent)\n",
        "    adapt_idx = idx_all[order[:k]]\n",
        "    test_idx  = idx_all[order[k:]]\n",
        "    return adapt_idx, test_idx\n",
        "\n",
        "def tune_weight_and_threshold(p_t, p_x, y_true, weights=None, thresholds=None):\n",
        "    if len(p_t)==0 or len(y_true)==0:\n",
        "        return (-1.0, 1.0, 0.5)\n",
        "    if weights is None: weights = np.linspace(0.0, 1.0, 11)\n",
        "    if thresholds is None: thresholds = np.linspace(0.2, 0.8, 25)\n",
        "    best = (-1.0, 1.0, 0.5)\n",
        "    for w in weights:\n",
        "        mix = w * p_t + (1 - w) * (p_x if p_x is not None and len(p_x)==len(p_t) else 0.0)\n",
        "        for thr in thresholds:\n",
        "            preds = (mix >= thr).astype(int)\n",
        "            f1 = f1_score(y_true, preds, average='macro')\n",
        "            if f1 > best[0]: best = (f1, w, thr)\n",
        "    return best\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def adapt_transformer_fewshot(model, ds_all, adapt_idx, lr, epochs):\n",
        "    dl = make_loader(ds_all, batch_size=Config.BATCH_SIZE, sampler=SubsetRandomSampler(adapt_idx))\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    steps = max(1, len(dl)*epochs)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, int(0.06*steps), steps)\n",
        "    scaler_amp = GradScaler()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); y=b['labels'].to(DEVICE)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                logits = model(input_ids=ids, attention_mask=am, return_dict=True).logits\n",
        "                loss = loss_fn(logits, y)\n",
        "            scaler_amp.scale(loss).backward()\n",
        "            scaler_amp.step(optimizer); scaler_amp.update(); scheduler.step()\n",
        "\n",
        "def adapt_fusion_fewshot(model, ds_all, adapt_idx, lr, epochs):\n",
        "    dl = make_loader(ds_all, batch_size=Config.BATCH_SIZE, sampler=SubsetRandomSampler(adapt_idx))\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    scaler_amp = GradScaler()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); y=b['labels'].to(DEVICE); feats=b['feats'].to(DEVICE)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                logits = model(ids, am, feats)\n",
        "                loss = loss_fn(logits, y)\n",
        "            scaler_amp.scale(loss).backward()\n",
        "            scaler_amp.step(optimizer); scaler_amp.update()\n",
        "\n",
        "print(\"Block E ready ✔️\")\n"
      ],
      "metadata": {
        "id": "9VJQaAtGqQ70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block F — Fit scalers on train, init XGB + Fusion (no leakage)\n",
        "# ============================================\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"Extracting features for in-domain training (XGB & fusion scalers)…\")\n",
        "feats_tr_raw = fe.extract_df(train_df)\n",
        "y_tr = train_df['label'].astype(int).values\n",
        "\n",
        "# XGB scaler (fit on train only)\n",
        "xgb_scaler = StandardScaler()\n",
        "feats_tr_scaled = xgb_scaler.fit_transform(feats_tr_raw)\n",
        "\n",
        "# XGB baseline\n",
        "try:\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        n_estimators=1500,\n",
        "        objective=\"binary:logistic\" if Config.NUM_LABELS == 2 else \"multi:softprob\",\n",
        "        num_class=(Config.NUM_LABELS if Config.NUM_LABELS>2 else None),\n",
        "        tree_method=\"hist\", n_jobs=4, random_state=13,\n",
        "        eval_metric=\"logloss\" if Config.NUM_LABELS==2 else \"mlogloss\"\n",
        "    )\n",
        "    xgb_model.fit(feats_tr_scaled, y_tr)\n",
        "    print(\"XGBoost baseline trained.\")\n",
        "except Exception as e:\n",
        "    xgb_model = None\n",
        "    print(\"⚠️ xgboost not installed or failed; skipping XGB baseline.\", e)\n",
        "\n",
        "# Fusion scalers (fit on train only; persist in RUN_DIR)\n",
        "os.makedirs(os.path.dirname(Config.SIMPLE_SCALER_PATH), exist_ok=True)\n",
        "os.makedirs(os.path.dirname(Config.GATED_SCALER_PATH), exist_ok=True)\n",
        "\n",
        "if os.path.isfile(Config.SIMPLE_SCALER_PATH):\n",
        "    simple_scaler = joblib.load(Config.SIMPLE_SCALER_PATH)\n",
        "else:\n",
        "    simple_scaler = StandardScaler().fit(feats_tr_raw)\n",
        "    joblib.dump(simple_scaler, Config.SIMPLE_SCALER_PATH)\n",
        "\n",
        "if os.path.isfile(Config.GATED_SCALER_PATH):\n",
        "    gated_scaler = joblib.load(Config.GATED_SCALER_PATH)\n",
        "else:\n",
        "    gated_scaler = StandardScaler().fit(feats_tr_raw)\n",
        "    joblib.dump(gated_scaler, Config.GATED_SCALER_PATH)\n",
        "\n",
        "# Init fusion models; load any saved heads if present\n",
        "simple = SimpleFusion(Config.MODEL_NAME, Config.FEAT_DIM, n_labels=Config.NUM_LABELS).to(DEVICE)\n",
        "gated  = GatedFusion(Config.MODEL_NAME, Config.FEAT_DIM, n_labels=Config.NUM_LABELS, feat_proj=64).to(DEVICE)\n",
        "\n",
        "if os.path.isfile(Config.SIMPLE_PT_PATH):\n",
        "    sd = torch.load(Config.SIMPLE_PT_PATH, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n",
        "    simple.load_state_dict(sd, strict=False)\n",
        "\n",
        "if os.path.isfile(Config.GATED_PT_PATH):\n",
        "    sd = torch.load(Config.GATED_PT_PATH, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n",
        "    gated.load_state_dict(sd, strict=False)\n",
        "\n",
        "print(\"Block F ready ✔️\")\n"
      ],
      "metadata": {
        "id": "GOVfcg5JqUWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block G — FT if needed, per-domain DAPT + adaptation loop, save\n",
        "# ============================================\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 1) Supervised FT (train once if no checkpoint)\n",
        "if not os.path.isfile(Config.XLM_R_MODEL_PATH):\n",
        "    print(\"🧪 Training base classifier (supervised FT)…\")\n",
        "    train_transformer_supervised(train_df, tokenizer, num_labels=Config.NUM_LABELS)\n",
        "else:\n",
        "    print(\"ℹ️ Found existing fine-tuned checkpoint; skipping FT.\")\n",
        "\n",
        "results: List[Dict[str, Any]] = []\n",
        "domains = ood_df['domain'].unique()\n",
        "\n",
        "for domain in domains:\n",
        "    print(f\"\\n--- Processing domain: {domain} ---\")\n",
        "    seed_everything(13)                    # reproducibility per domain\n",
        "    torch.cuda.empty_cache()               # avoid memory fragmentation\n",
        "\n",
        "    ddf = ood_df[ood_df['domain']==domain].copy().reset_index(drop=True)\n",
        "    y_true = ddf['label'].astype(int).values\n",
        "\n",
        "    # DAPT: train per-domain if missing\n",
        "    dapt_dir = os.path.join(Config.DAPT_SAVE_DIR, domain.replace(\" \", \"_\"))\n",
        "    if not os.path.isfile(os.path.join(dapt_dir, \"pytorch_model.bin\")):\n",
        "        print(f\"🧪 Running DAPT (MLM) for domain '{domain}' …\")\n",
        "        run_mlm_corpus_training(\n",
        "            ddf['content'].astype(str).tolist(),\n",
        "            save_dir=dapt_dir,\n",
        "            epochs=Config.DAPT_EPOCHS,\n",
        "            batch=Config.DAPT_BATCH,\n",
        "            lr=Config.DAPT_LR,\n",
        "            warmup_ratio=Config.DAPT_WARMUP_RATIO,\n",
        "            max_steps=Config.DAPT_MAX_STEPS,\n",
        "            tokenizer=tokenizer,\n",
        "        )\n",
        "\n",
        "    # Build DAPT+head classifier (robust encoder copy)\n",
        "    xlm_r = safe_load_seqcls_from_dapt(\n",
        "        dapt_dir=dapt_dir,\n",
        "        num_labels=Config.NUM_LABELS,\n",
        "        base_model_name=Config.MODEL_NAME\n",
        "    )\n",
        "    load_ft_classifier_only_into(xlm_r, Config.XLM_R_MODEL_PATH)\n",
        "    xlm_r.to(DEVICE)\n",
        "\n",
        "    # Push DAPT into fusion encoders (robust)\n",
        "    load_dapt_into_fusion_encoder(simple, dapt_dir, Config.MODEL_NAME)\n",
        "    load_dapt_into_fusion_encoder(gated,  dapt_dir, Config.MODEL_NAME)\n",
        "\n",
        "    # Build OOD datasets\n",
        "    feats_ood_raw = fe.extract_df(ddf)\n",
        "    ds_all        = TextFeatDataset(ddf, tokenizer, feats=None)\n",
        "    ds_simple_all = TextFeatDataset(ddf, tokenizer, feats=simple_scaler.transform(feats_ood_raw))\n",
        "    ds_gated_all  = TextFeatDataset(ddf, tokenizer, feats=gated_scaler.transform(feats_ood_raw))\n",
        "\n",
        "    n = len(ddf)\n",
        "    k_target = max(int(Config.DA_ADAPT_RATIO * n), 64)\n",
        "    k = min(k_target, max(0, n//2))\n",
        "\n",
        "    # ✅ Select *separate* uncertain sets per model (better adaptation for each)\n",
        "    proba_t = lambda idxs: get_probs_transformer(xlm_r, ds_all, idxs, batch=Config.BATCH_SIZE)\n",
        "    adapt_t, test_t = pick_uncertain_indices_with_proba(proba_t, k=k, n=n)\n",
        "    print(f\"[Transformer] Adapt: {len(adapt_t)} | Test: {len(test_t)}\")\n",
        "\n",
        "    proba_s = lambda idxs: get_probs_fusion(simple, ds_simple_all, idxs, batch=Config.BATCH_SIZE)\n",
        "    adapt_s, test_s = pick_uncertain_indices_with_proba(proba_s, k=k, n=n)\n",
        "    print(f\"[SimpleFusion] Adapt: {len(adapt_s)} | Test: {len(test_s)}\")\n",
        "\n",
        "    proba_g = lambda idxs: get_probs_fusion(gated, ds_gated_all, idxs, batch=Config.BATCH_SIZE)\n",
        "    adapt_g, test_g = pick_uncertain_indices_with_proba(proba_g, k=k, n=n)\n",
        "    print(f\"[GatedFusion] Adapt: {len(adapt_g)} | Test: {len(test_g)}\")\n",
        "\n",
        "    # ---- Zero-shot: transformer\n",
        "    f1 = evaluate_model(xlm_r, make_loader(ds_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(np.arange(n))), 'transformer')\n",
        "    results.append({'Domain':domain,'Model':'XLM-R Only','Evaluation':'Zero-Shot','Macro F1-Score':float(f1)})\n",
        "\n",
        "    # ---- Zero-shot: XGB\n",
        "    if 'xgb_model' in globals() and xgb_model is not None:\n",
        "        preds = xgb_model.predict(xgb_scaler.transform(feats_ood_raw))\n",
        "        f1 = f1_score(y_true, preds, average='macro')\n",
        "        results.append({'Domain':domain,'Model':'XGBoost + Features','Evaluation':'Zero-Shot','Macro F1-Score':float(f1)})\n",
        "\n",
        "    # ---- Zero-shot: fusion (DAPT encoders)\n",
        "    f1 = evaluate_model(simple, make_loader(ds_simple_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(np.arange(n))), 'simple')\n",
        "    results.append({'Domain':domain,'Model':'Simple Fusion','Evaluation':'Zero-Shot (DAPT encoder)','Macro F1-Score':float(f1)})\n",
        "\n",
        "    f1 = evaluate_model(gated, make_loader(ds_gated_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(np.arange(n))), 'gated')\n",
        "    results.append({'Domain':domain,'Model':'Gated Fusion','Evaluation':'Zero-Shot (DAPT encoder)','Macro F1-Score':float(f1)})\n",
        "\n",
        "    # ---- Zero-shot Ensemble tuned on transformer-adapt split (consistent)\n",
        "    if 'xgb_model' in globals() and xgb_model is not None and len(adapt_t)>0 and len(test_t)>0:\n",
        "        p_t_adapt = get_probs_transformer(xlm_r, ds_all, adapt_t, batch=Config.BATCH_SIZE)\n",
        "        p_x_adapt = xgb_model.predict_proba(xgb_scaler.transform(feats_ood_raw[adapt_t]))[:,1]\n",
        "        best_f1, best_w, best_thr = tune_weight_and_threshold(p_t_adapt, p_x_adapt, y_true[adapt_t])\n",
        "\n",
        "        p_t_test = get_probs_transformer(xlm_r, ds_all, test_t, batch=Config.BATCH_SIZE)\n",
        "        p_x_test = xgb_model.predict_proba(xgb_scaler.transform(feats_ood_raw[test_t]))[:,1]\n",
        "        p_blend = best_w*p_t_test + (1-best_w)*p_x_test\n",
        "        preds = (p_blend >= best_thr).astype(int)\n",
        "        f1_blend = f1_score(y_true[test_t], preds, average='macro')\n",
        "        results.append({'Domain':domain,'Model':f'Ensemble(T+XGB) w={best_w:.2f} thr={best_thr:.2f}',\n",
        "                        'Evaluation':'Zero-Shot (tuned on adapt)','Macro F1-Score':float(f1_blend)})\n",
        "\n",
        "    # ---- Domain Adaptation: transformer + fusion (each on its own adapt set)\n",
        "    if len(adapt_t)>0 and len(test_t)>0:\n",
        "        print(\"  Running Domain Adaptation…\")\n",
        "        unfreeze_top_n(xlm_r, Config.DA_UNFREEZE_TOP_N)\n",
        "        adapt_transformer_fewshot(xlm_r, ds_all, adapt_t, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "        f1 = evaluate_model(xlm_r, make_loader(ds_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(test_t)), 'transformer')\n",
        "        results.append({'Domain':domain,'Model':'XLM-R Only','Evaluation':'Domain Adapted','Macro F1-Score':float(f1)})\n",
        "\n",
        "    if len(adapt_s)>0 and len(test_s)>0:\n",
        "        unfreeze_top_n(simple, Config.DA_UNFREEZE_TOP_N)\n",
        "        adapt_fusion_fewshot(simple, ds_simple_all, adapt_s, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "        f1 = evaluate_model(simple, make_loader(ds_simple_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(test_s)), 'simple')\n",
        "        results.append({'Domain':domain,'Model':'Simple Fusion','Evaluation':'Domain Adapted','Macro F1-Score':float(f1)})\n",
        "\n",
        "    if len(adapt_g)>0 and len(test_g)>0:\n",
        "        unfreeze_top_n(gated, Config.DA_UNFREEZE_TOP_N)\n",
        "        adapt_fusion_fewshot(gated, ds_gated_all, adapt_g, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "        f1 = evaluate_model(gated, make_loader(ds_gated_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(test_g)), 'gated')\n",
        "        results.append({'Domain':domain,'Model':'Gated Fusion','Evaluation':'Domain Adapted','Macro F1-Score':float(f1)})\n",
        "\n",
        "    # Optional: ensemble after adaptation (still using transformer split for comparability)\n",
        "    if 'xgb_model' in globals() and xgb_model is not None and len(adapt_t)>0 and len(test_t)>0:\n",
        "        p_t_adapt2 = get_probs_transformer(xlm_r, ds_all, adapt_t, batch=Config.BATCH_SIZE)\n",
        "        p_x_adapt2 = xgb_model.predict_proba(xgb_scaler.transform(feats_ood_raw[adapt_t]))[:,1]\n",
        "        best_f1, best_w, best_thr = tune_weight_and_threshold(p_t_adapt2, p_x_adapt2, y_true[adapt_t])\n",
        "\n",
        "        p_t_test2 = get_probs_transformer(xlm_r, ds_all, test_t, batch=Config.BATCH_SIZE)\n",
        "        p_x_test2 = xgb_model.predict_proba(xgb_scaler.transform(feats_ood_raw[test_t]))[:,1]\n",
        "        p_blend2 = best_w*p_t_test2 + (1-best_w)*p_x_test2\n",
        "        preds2 = (p_blend2 >= best_thr).astype(int)\n",
        "        f1_blend2 = f1_score(y_true[test_t], preds2, average='macro')\n",
        "        results.append({'Domain':domain,'Model':f'Ensemble(T+XGB) w={best_w:.2f} thr={best_thr:.2f}',\n",
        "                        'Evaluation':'Domain Adapted (tuned)','Macro F1-Score':float(f1_blend2)})\n",
        "\n",
        "# Save & pretty print\n",
        "results_df = pd.DataFrame(results)\n",
        "os.makedirs(os.path.dirname(Config.RESULTS_CSV_PATH), exist_ok=True)\n",
        "results_df.to_csv(Config.RESULTS_CSV_PATH, index=False)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"CROSS-DOMAIN GENERALIZATION FINAL RESULTS\")\n",
        "print(\"=\"*80)\n",
        "from pandas import option_context\n",
        "with option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(nice_pivot(results_df))\n",
        "print(f\"\\n✅ Results summary saved to: {Config.RESULTS_CSV_PATH}\")\n",
        "\n",
        "print(\"Block G done ✔️\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V92mnWbtqeyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Sanity Checks v2: fixed splits + class weighting + threshold calibration\n",
        "# Run AFTER Blocks A–G (where models, configs, datasets are defined)\n",
        "# ============================================\n",
        "\n",
        "import numpy as np, pandas as pd, os, torch\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------- Utilities from the pipeline (light wrappers) ----------\n",
        "def _predict_argmax_transformer(model, ds, idx, batch=64):\n",
        "    if len(idx) == 0: return np.array([], dtype=int)\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out=[]\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE)\n",
        "            logits = model(input_ids=ids, attention_mask=am, return_dict=True).logits\n",
        "            out.append(logits.argmax(1).cpu().numpy())\n",
        "    return np.concatenate(out) if out else np.array([], dtype=int)\n",
        "\n",
        "def _predict_argmax_fusion(model, ds, idx, batch=64):\n",
        "    if len(idx) == 0: return np.array([], dtype=int)\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out=[]\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); feats=b['feats'].to(DEVICE)\n",
        "            logits = model(input_ids=ids, attention_mask=am, feats=feats)\n",
        "            out.append(logits.argmax(1).cpu().numpy())\n",
        "    return np.concatenate(out) if out else np.array([], dtype=int)\n",
        "\n",
        "def _get_probs_transformer(model, ds, idx, batch=64):\n",
        "    if Config.NUM_LABELS != 2: raise ValueError(\"Threshold calibration only coded for binary.\")\n",
        "    if len(idx) == 0: return np.array([])\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out=[]\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE)\n",
        "            p = torch.softmax(model(input_ids=ids, attention_mask=am, return_dict=True).logits, dim=-1)[:,1]\n",
        "            out.append(p.cpu().numpy())\n",
        "    return np.concatenate(out) if out else np.array([])\n",
        "\n",
        "def _get_probs_fusion(model, ds, idx, batch=64):\n",
        "    if Config.NUM_LABELS != 2: raise ValueError(\"Threshold calibration only coded for binary.\")\n",
        "    if len(idx) == 0: return np.array([])\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out=[]\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); feats=b['feats'].to(DEVICE)\n",
        "            p = torch.softmax(model(input_ids=ids, attention_mask=am, feats=feats), dim=-1)[:,1]\n",
        "            out.append(p.cpu().numpy())\n",
        "    return np.concatenate(out) if out else np.array([])\n",
        "\n",
        "def _bootstrap_ci_macro_f1(y_true, y_pred, n_boot=1000, seed=123):\n",
        "    if len(y_true) == 0: return (float(\"nan\"), float(\"nan\"))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true); scores = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        scores.append(f1_score(y_true[idx], y_pred[idx], average='macro'))\n",
        "    lo, hi = np.percentile(scores, [2.5, 97.5])\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "def _entropy(p):\n",
        "    p = np.clip(p, 1e-9, 1-1e-9)\n",
        "    return -(p*np.log(p) + (1-p)*np.log(1-p))\n",
        "\n",
        "def _calibrate_threshold(p, y, grid=None):\n",
        "    # maximize macro-F1 on adapt set\n",
        "    if grid is None: grid = np.linspace(0.2, 0.8, 61)\n",
        "    best = (0.0, 0.5)\n",
        "    for t in grid:\n",
        "        preds = (p >= t).astype(int)\n",
        "        f1 = f1_score(y, preds, average='macro')\n",
        "        if f1 > best[0]: best = (f1, t)\n",
        "    return best[1]\n",
        "\n",
        "def _class_weights_from_labels(y, num_classes):\n",
        "    counts = np.bincount(y, minlength=num_classes).astype(np.float32)\n",
        "    w = counts.sum() / np.maximum(1.0, counts)\n",
        "    w = w / w.mean()\n",
        "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "# ---------- Weighted adaptation (transformer + fusion) ----------\n",
        "def adapt_transformer_weighted(model, ds_all, adapt_idx, lr=1.5e-5, epochs=4, label_smoothing=0.05):\n",
        "    from torch.optim import AdamW\n",
        "    from transformers import get_linear_schedule_with_warmup\n",
        "    from torch.amp import autocast, GradScaler\n",
        "\n",
        "    y_adapt = ds_all.labels[adapt_idx]\n",
        "    ce = torch.nn.CrossEntropyLoss(weight=_class_weights_from_labels(y_adapt, Config.NUM_LABELS),\n",
        "                                   label_smoothing=label_smoothing)\n",
        "    opt = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    steps = max(1, math.ceil(len(adapt_idx)/max(1, Config.BATCH_SIZE)))*epochs\n",
        "    sch = get_linear_schedule_with_warmup(opt, 0, steps)\n",
        "    scaler = GradScaler(device=\"cuda\") if DEVICE==\"cuda\" else GradScaler()\n",
        "    dl = DataLoader(ds_all, batch_size=Config.BATCH_SIZE, sampler=SubsetRandomSampler(adapt_idx))\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for b in dl:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); y=b['labels'].to(DEVICE)\n",
        "            with autocast(device_type=\"cuda\") if DEVICE==\"cuda\" else torch.cuda.amp.autocast(enabled=False):\n",
        "                loss = ce(model(input_ids=ids, attention_mask=am, return_dict=True).logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt); scaler.update(); sch.step()\n",
        "\n",
        "def adapt_fusion_weighted(model, ds_all, adapt_idx, lr=1.5e-5, epochs=4, label_smoothing=0.05):\n",
        "    from torch.optim import AdamW\n",
        "    from transformers import get_linear_schedule_with_warmup\n",
        "    from torch.amp import autocast, GradScaler\n",
        "\n",
        "    y_adapt = ds_all.labels[adapt_idx]\n",
        "    ce = torch.nn.CrossEntropyLoss(weight=_class_weights_from_labels(y_adapt, Config.NUM_LABELS),\n",
        "                                   label_smoothing=label_smoothing)\n",
        "    opt = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    steps = max(1, math.ceil(len(adapt_idx)/max(1, Config.BATCH_SIZE)))*epochs\n",
        "    sch = get_linear_schedule_with_warmup(opt, 0, steps)\n",
        "    scaler = GradScaler(device=\"cuda\") if DEVICE==\"cuda\" else GradScaler()\n",
        "    dl = DataLoader(ds_all, batch_size=Config.BATCH_SIZE, sampler=SubsetRandomSampler(adapt_idx))\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for b in dl:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); feats=b['feats'].to(DEVICE); y=b['labels'].to(DEVICE)\n",
        "            with autocast(device_type=\"cuda\") if DEVICE==\"cuda\" else torch.cuda.amp.autocast(enabled=False):\n",
        "                loss = ce(model(input_ids=ids, attention_mask=am, feats=feats), y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt); scaler.update(); sch.step()\n",
        "\n",
        "# ---------- Precompute FIXED adapt/test splits (once) ----------\n",
        "def make_fixed_splits(entropy_seed=13):\n",
        "    fixed = {}\n",
        "    rng = np.random.default_rng(entropy_seed)\n",
        "    for domain in ood_df['domain'].unique():\n",
        "        ddf = ood_df[ood_df['domain']==domain].copy().reset_index(drop=True)\n",
        "        n = len(ddf)\n",
        "        feats = fe.extract_df(ddf)\n",
        "        ds_all = TextFeatDataset(ddf, tokenizer, feats=None)\n",
        "\n",
        "        # transformer with deterministic seed for split\n",
        "        seed_everything(entropy_seed)\n",
        "        dapt_dir = os.path.join(Config.DAPT_SAVE_DIR, domain.replace(\" \", \"_\"))\n",
        "        m = safe_load_seqcls_from_dapt(dapt_dir, Config.NUM_LABELS, Config.MODEL_NAME)\n",
        "        load_ft_classifier_only_into(m, Config.XLM_R_MODEL_PATH); m.to(DEVICE)\n",
        "\n",
        "        p = _get_probs_transformer(m, ds_all, np.arange(n), batch=Config.BATCH_SIZE)\n",
        "        ent = _entropy(p)\n",
        "\n",
        "        k_target = max(int(Config.DA_ADAPT_RATIO*n), 64)\n",
        "        k = min(k_target, max(1, n//2))\n",
        "        order = np.argsort(-ent)   # highest uncertainty first\n",
        "        adapt_idx = order[:k]\n",
        "        test_idx  = order[k:]\n",
        "        fixed[domain] = (adapt_idx, test_idx)\n",
        "    return fixed\n",
        "\n",
        "# ---------- Main runner with fixed splits + weighted adaptation + threshold calibration ----------\n",
        "def run_sanity_checks_v2(\n",
        "    seeds=(7,11,19),\n",
        "    models=(\"xlmr\",\"simple\",\"gated\"),\n",
        "    n_boot=1000,\n",
        "    fixed_splits=None\n",
        "):\n",
        "    if fixed_splits is None:\n",
        "        fixed_splits = make_fixed_splits(entropy_seed=13)\n",
        "\n",
        "    rows_summary=[]; rows_ci=[]; rows_perclass=[]\n",
        "\n",
        "    for domain in ood_df['domain'].unique():\n",
        "        ddf = ood_df[ood_df['domain']==domain].copy().reset_index(drop=True)\n",
        "        ddf = remap_labels_if_needed(ddf)\n",
        "        y_all = ddf['label'].astype(int).values\n",
        "        n = len(ddf)\n",
        "\n",
        "        feats_ood_raw = fe.extract_df(ddf)\n",
        "        ds_all = TextFeatDataset(ddf, tokenizer, feats=None)\n",
        "        ds_simple_all = TextFeatDataset(ddf, tokenizer, feats=simple_scaler.transform(feats_ood_raw)) if ('simple' in models and 'simple_scaler' in globals()) else None\n",
        "        ds_gated_all  = TextFeatDataset(ddf, tokenizer, feats=gated_scaler.transform(feats_ood_raw))  if ('gated'  in models and 'gated_scaler'  in globals()) else None\n",
        "\n",
        "        adapt_idx, test_idx = fixed_splits[domain]\n",
        "        per_seed = {m:[] for m in models}\n",
        "\n",
        "        for seed in seeds:\n",
        "            seed_everything(seed)\n",
        "            dapt_dir = os.path.join(Config.DAPT_SAVE_DIR, domain.replace(\" \", \"_\"))\n",
        "\n",
        "            # --- Transformer ---\n",
        "            if \"xlmr\" in models:\n",
        "                xlm = safe_load_seqcls_from_dapt(dapt_dir, Config.NUM_LABELS, Config.MODEL_NAME)\n",
        "                load_ft_classifier_only_into(xlm, Config.XLM_R_MODEL_PATH); xlm.to(DEVICE)\n",
        "                unfreeze_top_n(xlm, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_transformer_weighted(xlm, ds_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS, label_smoothing=0.05)\n",
        "\n",
        "                if Config.NUM_LABELS==2:\n",
        "                    p_adapt = _get_probs_transformer(xlm, ds_all, adapt_idx, batch=Config.BATCH_SIZE)\n",
        "                    thr = _calibrate_threshold(p_adapt, y_all[adapt_idx])\n",
        "                    p_test = _get_probs_transformer(xlm, ds_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                    y_pred = (p_test >= thr).astype(int)\n",
        "                else:\n",
        "                    y_pred = _predict_argmax_transformer(xlm, ds_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro'); per_seed[\"xlmr\"].append(f1)\n",
        "                lo,hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+101)\n",
        "                rows_ci.append({\"Domain\":domain,\"Model\":\"XLM-R Only (DA)\",\"Seed\":seed,\"Macro F1 (95% CI)\":f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"XLM-R Only (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "            # --- Simple Fusion ---\n",
        "            if ds_simple_all is not None:\n",
        "                simple = SimpleFusion(Config.MODEL_NAME, Config.FEAT_DIM).to(DEVICE)\n",
        "                load_dapt_into_fusion_encoder(simple, dapt_dir, Config.MODEL_NAME)\n",
        "                unfreeze_top_n(simple, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_fusion_weighted(simple, ds_simple_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS, label_smoothing=0.05)\n",
        "\n",
        "                if Config.NUM_LABELS==2:\n",
        "                    p_adapt = _get_probs_fusion(simple, ds_simple_all, adapt_idx, batch=Config.BATCH_SIZE)\n",
        "                    thr = _calibrate_threshold(p_adapt, y_all[adapt_idx])\n",
        "                    p_test = _get_probs_fusion(simple, ds_simple_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                    y_pred = (p_test >= thr).astype(int)\n",
        "                else:\n",
        "                    y_pred = _predict_argmax_fusion(simple, ds_simple_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro'); per_seed.setdefault(\"simple\",[]).append(f1)\n",
        "                lo,hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+202)\n",
        "                rows_ci.append({\"Domain\":domain,\"Model\":\"Simple Fusion (DA)\",\"Seed\":seed,\"Macro F1 (95% CI)\":f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"Simple Fusion (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "            # --- Gated Fusion ---\n",
        "            if ds_gated_all is not None:\n",
        "                gated = GatedFusion(Config.MODEL_NAME, Config.FEAT_DIM, feat_proj=64).to(DEVICE)\n",
        "                load_dapt_into_fusion_encoder(gated, dapt_dir, Config.MODEL_NAME)\n",
        "                unfreeze_top_n(gated, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_fusion_weighted(gated, ds_gated_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS, label_smoothing=0.05)\n",
        "\n",
        "                if Config.NUM_LABELS==2:\n",
        "                    p_adapt = _get_probs_fusion(gated, ds_gated_all, adapt_idx, batch=Config.BATCH_SIZE)\n",
        "                    thr = _calibrate_threshold(p_adapt, y_all[adapt_idx])\n",
        "                    p_test = _get_probs_fusion(gated, ds_gated_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                    y_pred = (p_test >= thr).astype(int)\n",
        "                else:\n",
        "                    y_pred = _predict_argmax_fusion(gated, ds_gated_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro'); per_seed.setdefault(\"gated\",[]).append(f1)\n",
        "                lo,hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+303)\n",
        "                rows_ci.append({\"Domain\":domain,\"Model\":\"Gated Fusion (DA)\",\"Seed\":seed,\"Macro F1 (95% CI)\":f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"Gated Fusion (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "        # summarize mean±std\n",
        "        for m, label in [(\"xlmr\",\"XLM-R Only (DA)\"), (\"simple\",\"Simple Fusion (DA)\"), (\"gated\",\"Gated Fusion (DA)\")]:\n",
        "            if m not in models: continue\n",
        "            arr = np.array(per_seed.get(m,[]), dtype=float)\n",
        "            if arr.size:\n",
        "                rows_summary.append({\"Domain\":domain,\"Model\":label,\"Seeds\":str(list(seeds)),\"Macro F1 (mean ± std)\":f\"{arr.mean():.4f} ± {arr.std(ddof=1):.4f}\"})\n",
        "\n",
        "    df_sum = pd.DataFrame(rows_summary).sort_values([\"Domain\",\"Model\"])\n",
        "    df_ci  = pd.DataFrame(rows_ci).sort_values([\"Domain\",\"Model\",\"Seed\"])\n",
        "    df_pc  = pd.DataFrame(rows_perclass).sort_values([\"Domain\",\"Model\",\"Seed\",\"Class\"])\n",
        "\n",
        "    out_dir = \"/content/emc_run\"; os.makedirs(out_dir, exist_ok=True)\n",
        "    df_sum.to_csv(os.path.join(out_dir,\"sanity_v2_seed_robustness_summary.csv\"), index=False)\n",
        "    df_ci.to_csv(os.path.join(out_dir,\"sanity_v2_bootstrap_cis.csv\"), index=False)\n",
        "    df_pc.to_csv(os.path.join(out_dir,\"sanity_v2_per_class_f1.csv\"), index=False)\n",
        "\n",
        "    print(\"\\n=== Seed robustness (Macro F1 mean ± std) — v2 ===\")\n",
        "    print(df_sum.to_string(index=False))\n",
        "    print(\"\\n=== Bootstrap 95% CI (per seed) — v2 ===\")\n",
        "    print(df_ci.to_string(index=False))\n",
        "    print(\"\\n=== Per-class F1 (per seed) — v2 ===\")\n",
        "    print(df_pc.to_string(index=False))\n",
        "    print(f\"\\n✅ Saved:\\n- {os.path.join(out_dir,'sanity_v2_seed_robustness_summary.csv')}\\n- {os.path.join(out_dir,'sanity_v2_bootstrap_cis.csv')}\\n- {os.path.join(out_dir,'sanity_v2_per_class_f1.csv')}\")\n",
        "\n",
        "# ---- Run it\n",
        "models_to_check = (\"xlmr\",\"simple\",\"gated\")   # adjust if needed\n",
        "fixed_splits = make_fixed_splits(entropy_seed=13)\n",
        "run_sanity_checks_v2(seeds=(7,11,19), models=models_to_check, n_boot=1000, fixed_splits=fixed_splits)\n"
      ],
      "metadata": {
        "id": "RVtmpB_JyDHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Sanity Checks: seeds + bootstrap\n",
        "# Run this AFTER Blocks A–G\n",
        "# ================================\n",
        "\n",
        "import numpy as np, pandas as pd, math, os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import torch\n",
        "\n",
        "# ---------- Helpers (predictions, selection, bootstrap) ----------\n",
        "def _predict_transformer(model, ds, idx, batch=64):\n",
        "    if len(idx) == 0: return np.array([], dtype=int)\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); preds = []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(DEVICE); am = b['attention_mask'].to(DEVICE)\n",
        "            out = model(input_ids=ids, attention_mask=am, return_dict=True)\n",
        "            p = out.logits.argmax(1).cpu().numpy()\n",
        "            preds.append(p)\n",
        "    return np.concatenate(preds) if preds else np.array([], dtype=int)\n",
        "\n",
        "def _predict_fusion(model, ds, idx, batch=64):\n",
        "    if len(idx) == 0: return np.array([], dtype=int)\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); preds = []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(DEVICE); am = b['attention_mask'].to(DEVICE); feats = b['feats'].to(DEVICE)\n",
        "            logits = model(input_ids=ids, attention_mask=am, feats=feats)\n",
        "            p = logits.argmax(1).cpu().numpy()\n",
        "            preds.append(p)\n",
        "    return np.concatenate(preds) if preds else np.array([], dtype=int)\n",
        "\n",
        "def _entropy_from_probs_binary(p):\n",
        "    p = np.clip(p, 1e-9, 1 - 1e-9)\n",
        "    return -(p * np.log(p) + (1 - p) * np.log(1 - p))\n",
        "\n",
        "def _pick_uncertain_by_entropy(proba_fn, n, k):\n",
        "    \"\"\"proba_fn: function(idxs)->probs_of_class1 (np.ndarray)\"\"\"\n",
        "    idx_all = np.arange(n)\n",
        "    probs = proba_fn(idx_all)\n",
        "    ent = _entropy_from_probs_binary(probs)\n",
        "    order = np.argsort(-ent)          # high entropy = more uncertain\n",
        "    k = min(max(k, 1), max(1, n - 1)) # ensure non-empty test\n",
        "    adapt_idx = idx_all[order[:k]]\n",
        "    test_idx  = idx_all[order[k:]]\n",
        "    return adapt_idx, test_idx\n",
        "\n",
        "def _bootstrap_ci_macro_f1(y_true, y_pred, n_boot=1000, seed=123):\n",
        "    if len(y_true) == 0: return (float(\"nan\"), float(\"nan\"))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true)\n",
        "    scores = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        scores.append(f1_score(y_true[idx], y_pred[idx], average='macro'))\n",
        "    lo, hi = np.percentile(scores, [2.5, 97.5])\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "# ---------- Minimal fallbacks for fusion adaptation if not defined ----------\n",
        "try:\n",
        "    adapt_fusion_fewshot\n",
        "except NameError:\n",
        "    def adapt_fusion_fewshot(model, ds_all, adapt_idx, lr=1.5e-5, epochs=4):\n",
        "        from torch.optim import AdamW\n",
        "        from transformers import get_linear_schedule_with_warmup\n",
        "        from torch.amp import GradScaler, autocast\n",
        "        scaler = GradScaler(device=\"cuda\") if DEVICE==\"cuda\" else GradScaler()\n",
        "        opt = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "        steps = max(1, len(adapt_idx) // max(1, Config.BATCH_SIZE)) * epochs\n",
        "        sch = get_linear_schedule_with_warmup(opt, 0, steps)\n",
        "        ce = torch.nn.CrossEntropyLoss()\n",
        "        dl = DataLoader(ds_all, batch_size=Config.BATCH_SIZE,\n",
        "                        sampler=SubsetRandomSampler(adapt_idx), drop_last=False)\n",
        "        model.train()\n",
        "        for _ in range(epochs):\n",
        "            for b in dl:\n",
        "                ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE)\n",
        "                feats=b['feats'].to(DEVICE); y=b['labels'].to(DEVICE)\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                with autocast(device_type=\"cuda\") if DEVICE==\"cuda\" else torch.cuda.amp.autocast(enabled=False):\n",
        "                    logits = model(input_ids=ids, attention_mask=am, feats=feats)\n",
        "                    loss = ce(logits, y)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(opt); scaler.update(); sch.step()\n",
        "\n",
        "try:\n",
        "    get_probs_fusion\n",
        "except NameError:\n",
        "    def get_probs_fusion(model, ds, idx, batch=64):\n",
        "        if len(idx) == 0: return np.array([])\n",
        "        dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "        model.eval(); out=[]\n",
        "        with torch.no_grad():\n",
        "            for b in dl:\n",
        "                ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); feats=b['feats'].to(DEVICE)\n",
        "                logits = model(input_ids=ids, attention_mask=am, feats=feats)\n",
        "                p = torch.softmax(logits, dim=-1)[:, 1].cpu().numpy()\n",
        "                out.append(p)\n",
        "        return np.concatenate(out) if len(out)>0 else np.array([])\n",
        "\n",
        "# ---------- Main sanity runner ----------\n",
        "def run_sanity_checks(\n",
        "    seeds=(7, 11, 19),\n",
        "    models=(\"xlmr\", \"simple\", \"gated\"),   # choose any subset\n",
        "    n_boot=1000\n",
        "):\n",
        "    domains = ood_df['domain'].unique()\n",
        "    rows_summary = []\n",
        "    rows_ci = []\n",
        "    rows_perclass = []\n",
        "\n",
        "    for domain in domains:\n",
        "        print(f\"\\n=== Sanity checks for domain: {domain} ===\")\n",
        "        ddf = ood_df[ood_df['domain']==domain].copy().reset_index(drop=True)\n",
        "        y_all = ddf['label'].astype(int).values\n",
        "        n = len(ddf)\n",
        "\n",
        "        # Build fixed datasets once per domain\n",
        "        feats_ood_raw = fe.extract_df(ddf)\n",
        "        ds_all = TextFeatDataset(ddf, tokenizer, feats=None)\n",
        "\n",
        "        ds_simple_all = None; ds_gated_all = None\n",
        "        have_simple = (\"simple\" in models) and ('simple_scaler' in globals())\n",
        "        have_gated  = (\"gated\"  in models) and ('gated_scaler'  in globals())\n",
        "\n",
        "        if have_simple:\n",
        "            ds_simple_all = TextFeatDataset(ddf, tokenizer, feats=simple_scaler.transform(feats_ood_raw))\n",
        "        if have_gated:\n",
        "            ds_gated_all = TextFeatDataset(ddf, tokenizer, feats=gated_scaler.transform(feats_ood_raw))\n",
        "\n",
        "        # compute adapt size\n",
        "        k_target = max(int(Config.DA_ADAPT_RATIO * n), 64)\n",
        "        k = min(k_target, max(1, n//2))\n",
        "\n",
        "        # collect per-seed results\n",
        "        per_seed_scores = {m: [] for m in models}\n",
        "\n",
        "        for seed in seeds:\n",
        "            seed_everything(seed)\n",
        "\n",
        "            dapt_dir = os.path.join(Config.DAPT_SAVE_DIR, domain.replace(\" \", \"_\"))\n",
        "\n",
        "            # XLM-R model (DAPT encoder + FT head)\n",
        "            xlm_r = safe_load_seqcls_from_dapt(dapt_dir, Config.NUM_LABELS, Config.MODEL_NAME)\n",
        "            load_ft_classifier_only_into(xlm_r, Config.XLM_R_MODEL_PATH)\n",
        "            xlm_r.to(DEVICE)\n",
        "\n",
        "            # Fusion models (fresh instances per seed)\n",
        "            simple_local = gated_local = None\n",
        "            if have_simple:\n",
        "                simple_local = SimpleFusion(Config.MODEL_NAME, Config.FEAT_DIM)\n",
        "                simple_local.to(DEVICE)\n",
        "                load_dapt_into_fusion_encoder(simple_local, dapt_dir, Config.MODEL_NAME)\n",
        "            if have_gated:\n",
        "                gated_local  = GatedFusion(Config.MODEL_NAME, Config.FEAT_DIM, feat_proj=64)\n",
        "                gated_local.to(DEVICE)\n",
        "                load_dapt_into_fusion_encoder(gated_local,  dapt_dir, Config.MODEL_NAME)\n",
        "\n",
        "            # Select adapt/test indices using Transformer uncertainty (consistent split)\n",
        "            p_all = get_probs_transformer(xlm_r, ds_all, np.arange(n), batch=Config.BATCH_SIZE)\n",
        "            adapt_idx, test_idx = _pick_uncertain_by_entropy(lambda idxs: p_all[idxs], n=n, k=k)\n",
        "\n",
        "            # ---- Adapt & evaluate: XLM-R\n",
        "            if \"xlmr\" in models:\n",
        "                unfreeze_top_n(xlm_r, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_transformer_fewshot(xlm_r, ds_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "                y_pred = _predict_transformer(xlm_r, ds_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro')\n",
        "                per_seed_scores[\"xlmr\"].append(f1)\n",
        "                # bootstrap CI & per-class\n",
        "                lo, hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+42)\n",
        "                rows_ci.append({\"Domain\": domain, \"Model\": \"XLM-R Only (DA)\", \"Seed\": seed, \"Macro F1 (95% CI)\": f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"XLM-R Only (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "            # ---- Adapt & evaluate: Simple Fusion\n",
        "            if have_simple and \"simple\" in models:\n",
        "                unfreeze_top_n(simple_local, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_fusion_fewshot(simple_local, ds_simple_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "                y_pred = _predict_fusion(simple_local, ds_simple_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro')\n",
        "                per_seed_scores[\"simple\"].append(f1)\n",
        "                lo, hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+99)\n",
        "                rows_ci.append({\"Domain\": domain, \"Model\": \"Simple Fusion (DA)\", \"Seed\": seed, \"Macro F1 (95% CI)\": f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"Simple Fusion (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "            # ---- Adapt & evaluate: Gated Fusion\n",
        "            if have_gated and \"gated\" in models:\n",
        "                unfreeze_top_n(gated_local, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_fusion_fewshot(gated_local, ds_gated_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "                y_pred = _predict_fusion(gated_local, ds_gated_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro')\n",
        "                per_seed_scores[\"gated\"].append(f1)\n",
        "                lo, hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+123)\n",
        "                rows_ci.append({\"Domain\": domain, \"Model\": \"Gated Fusion (DA)\", \"Seed\": seed, \"Macro F1 (95% CI)\": f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"Gated Fusion (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "        # Summarize mean±std across seeds\n",
        "        for m in models:\n",
        "            if len(per_seed_scores[m]) == 0: continue\n",
        "            arr = np.array(per_seed_scores[m], dtype=float)\n",
        "            rows_summary.append({\n",
        "                \"Domain\": domain,\n",
        "                \"Model\": {\"xlmr\":\"XLM-R Only (DA)\",\"simple\":\"Simple Fusion (DA)\",\"gated\":\"Gated Fusion (DA)\"}[m],\n",
        "                \"Seeds\": f\"{list(seeds)}\",\n",
        "                \"Macro F1 (mean ± std)\": f\"{arr.mean():.4f} ± {arr.std(ddof=1):.4f}\"\n",
        "            })\n",
        "\n",
        "    df_summary   = pd.DataFrame(rows_summary).sort_values([\"Domain\",\"Model\"]).reset_index(drop=True)\n",
        "    df_ci        = pd.DataFrame(rows_ci).sort_values([\"Domain\",\"Model\",\"Seed\"]).reset_index(drop=True)\n",
        "    df_perclass  = pd.DataFrame(rows_perclass).sort_values([\"Domain\",\"Model\",\"Seed\",\"Class\"]).reset_index(drop=True)\n",
        "\n",
        "    # Save & display\n",
        "    out_dir = os.path.join(\"/content\", \"emc_run\")\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    df_summary.to_csv(os.path.join(out_dir, \"sanity_seed_robustness_summary.csv\"), index=False)\n",
        "    df_ci.to_csv(os.path.join(out_dir, \"sanity_bootstrap_cis.csv\"), index=False)\n",
        "    df_perclass.to_csv(os.path.join(out_dir, \"sanity_per_class_f1.csv\"), index=False)\n",
        "\n",
        "    print(\"\\n=== Seed robustness (Macro F1 mean ± std) ===\")\n",
        "    print(df_summary.to_string(index=False))\n",
        "    print(\"\\n=== Bootstrap 95% CI (per seed) ===\")\n",
        "    print(df_ci.to_string(index=False))\n",
        "    print(\"\\n=== Per-class F1 (per seed) ===\")\n",
        "    print(df_perclass.to_string(index=False))\n",
        "    print(f\"\\n✅ Saved:\\n- {os.path.join(out_dir, 'sanity_seed_robustness_summary.csv')}\\n- {os.path.join(out_dir, 'sanity_bootstrap_cis.csv')}\\n- {os.path.join(out_dir, 'sanity_per_class_f1.csv')}\")\n",
        "\n",
        "# ---------- Run the sanity checks ----------\n",
        "# Choose which models to evaluate; comment out ones you don't need\n",
        "models_to_check = (\"xlmr\", \"simple\", \"gated\")   # or e.g., (\"xlmr\",\"simple\")\n",
        "run_sanity_checks(seeds=(7,11,19), models=models_to_check, n_boot=1000)\n"
      ],
      "metadata": {
        "id": "7AHVK1Kkwd3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create bar charts (with error bars) for your v2 seed-robust Macro F1 results\n",
        "# Requirements followed: matplotlib only, one chart per figure, no explicit colors/styles.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from caas_jupyter_tools import display_dataframe_to_user\n",
        "\n",
        "# ---- Seed-robust results (mean ± std across seeds {7,11,19}) ----\n",
        "data = [\n",
        "    # Domain, Model, mean, std\n",
        "    (\"Biomedical Engineering\", \"Gated Fusion (DA)\", 0.4631, 0.0956),\n",
        "    (\"Biomedical Engineering\", \"Simple Fusion (DA)\", 0.4773, 0.0628),\n",
        "    (\"Biomedical Engineering\", \"XLM-R Only (DA)\", 0.4582, 0.0355),\n",
        "    (\"Chemical Engineering\",   \"Gated Fusion (DA)\", 0.5036, 0.0696),\n",
        "    (\"Chemical Engineering\",   \"Simple Fusion (DA)\", 0.4842, 0.0412),\n",
        "    (\"Chemical Engineering\",   \"XLM-R Only (DA)\", 0.4655, 0.0329),\n",
        "    (\"Software Engineering\",   \"Gated Fusion (DA)\", 0.5053, 0.0635),\n",
        "    (\"Software Engineering\",   \"Simple Fusion (DA)\", 0.4882, 0.0526),\n",
        "    (\"Software Engineering\",   \"XLM-R Only (DA)\", 0.4689, 0.0440),\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"Domain\", \"Model\", \"MacroF1_mean\", \"MacroF1_std\"])\n",
        "\n",
        "# Show the data as a table in the UI\n",
        "display_dataframe_to_user(\"Seed-robust Macro F1 (mean ± std)\", df)\n",
        "\n",
        "# Ensure output directory\n",
        "out_dir = \"/mnt/data/figures\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# --- Helper to annotate bars ---\n",
        "def annotate_bars(ax, rects, values):\n",
        "    for rect, val in zip(rects, values):\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f\"{val:.3f}\",\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# --- Create a separate figure per domain ---\n",
        "saved_paths = []\n",
        "for domain in df[\"Domain\"].unique():\n",
        "    sub = df[df[\"Domain\"] == domain].copy()\n",
        "    # Keep a fixed model order for readability\n",
        "    model_order = [\"XLM-R Only (DA)\", \"Simple Fusion (DA)\", \"Gated Fusion (DA)\"]\n",
        "    sub[\"Model\"] = pd.Categorical(sub[\"Model\"], categories=model_order, ordered=True)\n",
        "    sub = sub.sort_values(\"Model\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    x = np.arange(len(sub))\n",
        "    bars = ax.bar(x, sub[\"MacroF1_mean\"].values, yerr=sub[\"MacroF1_std\"].values, capsize=5)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(sub[\"Model\"].tolist(), rotation=20, ha=\"right\")\n",
        "    ax.set_ylim(0.0, 1.0)\n",
        "    ax.set_ylabel(\"Macro F1\")\n",
        "    ax.set_title(f\"Out-of-Domain Performance after DA — {domain}\")\n",
        "    annotate_bars(ax, bars, sub[\"MacroF1_mean\"].values)\n",
        "\n",
        "    fname = f\"{domain.lower().replace(' ', '_')}_da_seed_robust_bars.png\"\n",
        "    fpath = os.path.join(out_dir, fname)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fpath, dpi=200, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    saved_paths.append(fpath)\n",
        "\n",
        "# Also produce a single combined figure with all domains grouped by model.\n",
        "# (Still one chart; each model has three bars—one per domain—with error bars)\n",
        "\n",
        "model_order = [\"XLM-R Only (DA)\", \"Simple Fusion (DA)\", \"Gated Fusion (DA)\"]\n",
        "domains = [\"Biomedical Engineering\", \"Chemical Engineering\", \"Software Engineering\"]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "width = 0.22\n",
        "x = np.arange(len(model_order))\n",
        "\n",
        "for i, domain in enumerate(domains):\n",
        "    sub = df[df[\"Domain\"] == domain].copy()\n",
        "    sub[\"Model\"] = pd.Categorical(sub[\"Model\"], categories=model_order, ordered=True)\n",
        "    sub = sub.sort_values(\"Model\")\n",
        "    means = sub[\"MacroF1_mean\"].values\n",
        "    stds = sub[\"MacroF1_std\"].values\n",
        "\n",
        "    rects = ax.bar(x + i*width - width, means, width, yerr=stds, capsize=4, label=domain)\n",
        "    annotate_bars(ax, rects, means)\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_order, rotation=15, ha=\"right\")\n",
        "ax.set_ylim(0.0, 1.0)\n",
        "ax.set_ylabel(\"Macro F1\")\n",
        "ax.set_title(\"Out-of-Domain Performance after DA — Seed-Robust Means ± SD\")\n",
        "ax.legend()\n",
        "\n",
        "combined_path = os.path.join(out_dir, \"combined_da_seed_robust_bars.png\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(combined_path, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "saved_paths.append(combined_path)\n",
        "\n",
        "saved_paths\n"
      ],
      "metadata": {
        "id": "nz5uVwWi19OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e05e67c"
      },
      "source": [
        "!pip install caas_jupyter_tools"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
