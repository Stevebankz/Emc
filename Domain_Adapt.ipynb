{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65fa2de1589440ee848d6a99b07a43f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcca5730a7c14b74844c7f8c186a90d2",
              "IPY_MODEL_756b06e4dca34afc97b757fa8b2089be",
              "IPY_MODEL_52b5969097884a94b769cc1b6f1cf45d"
            ],
            "layout": "IPY_MODEL_167bcec9dad0429594d0fbee3d3c204b"
          }
        },
        "bcca5730a7c14b74844c7f8c186a90d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c96fb7a074342fe8b85093909f6b636",
            "placeholder": "​",
            "style": "IPY_MODEL_d969692dc4014f3e8c2f3e4f94ae0263",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "756b06e4dca34afc97b757fa8b2089be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ae9cab0f4464ad0a715d72cbdd98187",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b865a055f1b841a59ff0ca500e99e87f",
            "value": 25
          }
        },
        "52b5969097884a94b769cc1b6f1cf45d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5978b54e1474cad983927e4f9c840cf",
            "placeholder": "​",
            "style": "IPY_MODEL_c1db91a55ccc42249559e2ba984a783d",
            "value": " 25.0/25.0 [00:00&lt;00:00, 3.17kB/s]"
          }
        },
        "167bcec9dad0429594d0fbee3d3c204b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c96fb7a074342fe8b85093909f6b636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d969692dc4014f3e8c2f3e4f94ae0263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ae9cab0f4464ad0a715d72cbdd98187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b865a055f1b841a59ff0ca500e99e87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5978b54e1474cad983927e4f9c840cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1db91a55ccc42249559e2ba984a783d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0140df75139e4c438984f366cc59f1ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8dc3869641fa4a3ba6c4209d0208f9a5",
              "IPY_MODEL_25a3fe0cfbbe4227be1261026116f082",
              "IPY_MODEL_85b450a8479c4af9b9fab3b47fe18008"
            ],
            "layout": "IPY_MODEL_91da408ecd11432cb0cdcd8d993e1326"
          }
        },
        "8dc3869641fa4a3ba6c4209d0208f9a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7593c8cab78e4c1b8d17ae5006b0ea43",
            "placeholder": "​",
            "style": "IPY_MODEL_b623fc2f25ec4f5d9aed17b8022aacf4",
            "value": "config.json: 100%"
          }
        },
        "25a3fe0cfbbe4227be1261026116f082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81ffc16020884158be2eb4332c550ad3",
            "max": 615,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d5e4eb60662440a9baad1ef68dcd84c",
            "value": 615
          }
        },
        "85b450a8479c4af9b9fab3b47fe18008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_002088ab548d45cb875508a120abfa14",
            "placeholder": "​",
            "style": "IPY_MODEL_3d532a3e5e594178a2b54b6aaa059de3",
            "value": " 615/615 [00:00&lt;00:00, 87.3kB/s]"
          }
        },
        "91da408ecd11432cb0cdcd8d993e1326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7593c8cab78e4c1b8d17ae5006b0ea43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b623fc2f25ec4f5d9aed17b8022aacf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81ffc16020884158be2eb4332c550ad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d5e4eb60662440a9baad1ef68dcd84c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "002088ab548d45cb875508a120abfa14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d532a3e5e594178a2b54b6aaa059de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7573ed97db194295ab41947973d68ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6739fa7b87cc438b829282f4ce7017e5",
              "IPY_MODEL_404f9c92a5124cbc9da4de5a1913dad6",
              "IPY_MODEL_da34c9883cb84daeb7eb9c04d53ca25d"
            ],
            "layout": "IPY_MODEL_ea9edc61451d449f8db48fa0307fcbed"
          }
        },
        "6739fa7b87cc438b829282f4ce7017e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de4b8884f1bf443f98a5f6b416d708ce",
            "placeholder": "​",
            "style": "IPY_MODEL_af6d307034ff45f89569b818fb13b5ab",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "404f9c92a5124cbc9da4de5a1913dad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a703924972d74f1a98928137a3e3652c",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4284e864c02f4b7dba3c246d183366b3",
            "value": 5069051
          }
        },
        "da34c9883cb84daeb7eb9c04d53ca25d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb9fd86dd2045b0adb44ba582c3490f",
            "placeholder": "​",
            "style": "IPY_MODEL_4cc84a329e064850b872457ebc480da1",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 16.4MB/s]"
          }
        },
        "ea9edc61451d449f8db48fa0307fcbed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de4b8884f1bf443f98a5f6b416d708ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af6d307034ff45f89569b818fb13b5ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a703924972d74f1a98928137a3e3652c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4284e864c02f4b7dba3c246d183366b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6eb9fd86dd2045b0adb44ba582c3490f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cc84a329e064850b872457ebc480da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbda1239d57c44d8a5ee46e4d0c35c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34fdad5d01314b3a8b6916f48b590cbb",
              "IPY_MODEL_6d9f4a8d65304d1393138d39fb7ed3f6",
              "IPY_MODEL_0d9fbca6f5a8444e8ee686e0487902a2"
            ],
            "layout": "IPY_MODEL_665520c2a9a74a73baaea1fcfe9fc038"
          }
        },
        "34fdad5d01314b3a8b6916f48b590cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b46c973367834ee79df57f5c0a33f3e2",
            "placeholder": "​",
            "style": "IPY_MODEL_48447e9716144be5a24d3dc06cf57a47",
            "value": "tokenizer.json: 100%"
          }
        },
        "6d9f4a8d65304d1393138d39fb7ed3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e32acfd2921e456f9e006f81c993d414",
            "max": 9096718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6cc74bdb28ed411782072c34c68d1da0",
            "value": 9096718
          }
        },
        "0d9fbca6f5a8444e8ee686e0487902a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55b29e335d6e47339c4f2aa7fa263ae0",
            "placeholder": "​",
            "style": "IPY_MODEL_f94101a886e0486394f8c54fc60d0b9c",
            "value": " 9.10M/9.10M [00:00&lt;00:00, 18.4MB/s]"
          }
        },
        "665520c2a9a74a73baaea1fcfe9fc038": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46c973367834ee79df57f5c0a33f3e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48447e9716144be5a24d3dc06cf57a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e32acfd2921e456f9e006f81c993d414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cc74bdb28ed411782072c34c68d1da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55b29e335d6e47339c4f2aa7fa263ae0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f94101a886e0486394f8c54fc60d0b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdd6c46d40a24a0fabedca74070a44c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_863600cb9a624380a15bbe8250c18c4c",
              "IPY_MODEL_227b48f09ee343febdde8b899c8ec9fa",
              "IPY_MODEL_78a0d8e6d2754208ad3e621405adac56"
            ],
            "layout": "IPY_MODEL_7fa52b6a479b4c95b840bf770e2ce8dc"
          }
        },
        "863600cb9a624380a15bbe8250c18c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d2ed8d274fa4aae8a6f923c4c1993ea",
            "placeholder": "​",
            "style": "IPY_MODEL_9b673ed14e5840559ff4870bf9851050",
            "value": "model.safetensors: 100%"
          }
        },
        "227b48f09ee343febdde8b899c8ec9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecc2593a3e8f43829575b5fbce9fab74",
            "max": 1115567652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53f7f837fa7241fa9a0f41965e207844",
            "value": 1115567652
          }
        },
        "78a0d8e6d2754208ad3e621405adac56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bd503285e554374b51bc8a7c6c604d6",
            "placeholder": "​",
            "style": "IPY_MODEL_7bf9cea8e8c24bd7b8e720662e39b73a",
            "value": " 1.12G/1.12G [00:02&lt;00:00, 1.05GB/s]"
          }
        },
        "7fa52b6a479b4c95b840bf770e2ce8dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d2ed8d274fa4aae8a6f923c4c1993ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b673ed14e5840559ff4870bf9851050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecc2593a3e8f43829575b5fbce9fab74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f7f837fa7241fa9a0f41965e207844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bd503285e554374b51bc8a7c6c604d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bf9cea8e8c24bd7b8e720662e39b73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VJnnwIPnFcLh",
        "outputId": "1d663cf6-7e1a-4e86-b469-5eb53c7dd886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.10-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textstat) (3.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading textstat-0.7.10-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.17.2 textstat-0.7.10\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m130.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers pandas scikit-learn tqdm textstat spacy accelerate xgboost\n",
        "\n",
        "# Install and download all at once\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1jHvWBLGC4E",
        "outputId": "44233430-c163-446a-a88f-da3a45486235"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block A — Setup + Load datasets (Drive in, RUN out)\n",
        "# ============================================\n",
        "\n",
        "# (optional) mount Drive first in Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import os, re, math, random, json, contextlib\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Torch / HF\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.use_deterministic_algorithms(False)\n",
        "try:\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Quiet AMP deprecation warnings with thin wrappers\n",
        "from torch.amp import autocast as _autocast, GradScaler as _GradScaler\n",
        "def autocast(enabled=True):\n",
        "    return _autocast(device_type=\"cuda\") if (enabled and torch.cuda.is_available() and enabled) \\\n",
        "           else contextlib.nullcontext()\n",
        "def GradScaler(**kw):\n",
        "    if torch.cuda.is_available():\n",
        "        return _GradScaler(device=\"cuda\", **kw)\n",
        "    return _GradScaler(**kw)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def seed_everything(seed: int = 13):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed);\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# -----------------------------\n",
        "# Config: READ from Drive; SAVE to /content/emc_run\n",
        "# -----------------------------\n",
        "class Config:\n",
        "    # READ inputs/artifacts FROM Drive (put your CSVs here)\n",
        "    DATA_DIR = \"/content/drive/MyDrive/emc\"\n",
        "\n",
        "    # SAVE all new artifacts/checkpoints TO local runtime (won't sync to Drive)\n",
        "    RUN_DIR  = \"/content/emc_run\"\n",
        "\n",
        "    # Data (read from Drive)\n",
        "    TRAIN_PATH     = os.path.join(DATA_DIR, \"train.csv\")\n",
        "    OOD_DATA_PATH  = os.path.join(DATA_DIR, \"ood_dataset.csv\")\n",
        "    TERMS_PATH     = os.path.join(DATA_DIR, \"engineering_terms.csv\")  # used later by FeatureExtractor\n",
        "\n",
        "    # Artifacts (save to /content/emc_run)\n",
        "    XLM_R_MODEL_PATH   = os.path.join(RUN_DIR, \"xlmr_only_outputs/pytorch_model.bin\")\n",
        "    SIMPLE_PT_PATH     = os.path.join(RUN_DIR, \"simple_fusion_outputs/fusion_simple.pt\")\n",
        "    GATED_PT_PATH      = os.path.join(RUN_DIR, \"gated_fusion_outputs/fusion_gated.pt\")\n",
        "    SIMPLE_SCALER_PATH = os.path.join(RUN_DIR, \"simple_fusion_outputs/scaler12.pkl\")\n",
        "    GATED_SCALER_PATH  = os.path.join(RUN_DIR, \"gated_fusion_outputs/scaler12.pkl\")\n",
        "    RESULTS_CSV_PATH   = os.path.join(RUN_DIR, \"domain_adaptation_results.csv\")\n",
        "    DAPT_SAVE_DIR      = os.path.join(RUN_DIR, \"dapt_mlm\")\n",
        "\n",
        "    # Model settings (will be reused later)\n",
        "    MODEL_NAME = \"xlm-roberta-base\"\n",
        "    MAX_LEN = 256\n",
        "    FEAT_DIM = 12\n",
        "    NUM_LABELS = 2  # will be overwritten after we infer\n",
        "\n",
        "    # Training/eval defaults (used later)\n",
        "    VAL_SPLIT = 0.1\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 2\n",
        "    PIN_MEMORY = (DEVICE == \"cuda\")\n",
        "\n",
        "    # Few-shot DA\n",
        "    DA_ADAPT_RATIO = 0.4\n",
        "    DA_EPOCHS = 4\n",
        "    DA_LR = 1.5e-5\n",
        "    DA_UNFREEZE_TOP_N = 16\n",
        "\n",
        "# Make sure local run dirs exist\n",
        "os.makedirs(Config.RUN_DIR, exist_ok=True)\n",
        "for p in [\n",
        "    os.path.dirname(Config.XLM_R_MODEL_PATH),\n",
        "    os.path.dirname(Config.SIMPLE_PT_PATH),\n",
        "    os.path.dirname(Config.GATED_PT_PATH),\n",
        "    os.path.dirname(Config.SIMPLE_SCALER_PATH),\n",
        "    os.path.dirname(Config.GATED_SCALER_PATH),\n",
        "    Config.DAPT_SAVE_DIR,\n",
        "]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Robust data loading helpers\n",
        "# -----------------------------\n",
        "def _require_file(path: str, name: str):\n",
        "    if not os.path.isfile(path):\n",
        "        raise FileNotFoundError(f\"Missing {name} at: {path}\")\n",
        "\n",
        "def _clean_text_series(s: pd.Series) -> pd.Series:\n",
        "    # ensure str; strip; replace NaN with \"\"\n",
        "    s = s.astype(str).fillna(\"\").map(lambda x: x.strip())\n",
        "    # collapse super long whitespace\n",
        "    s = s.map(lambda x: re.sub(r\"\\s+\", \" \", x))\n",
        "    return s\n",
        "\n",
        "def _ensure_lang_col(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if 'lang' not in df.columns:\n",
        "        df = df.copy()\n",
        "        df['lang'] = 'en'\n",
        "    return df\n",
        "\n",
        "def _drop_empty_rows(df: pd.DataFrame, text_col=\"content\") -> pd.DataFrame:\n",
        "    before = len(df)\n",
        "    df = df[df[text_col].astype(str).str.strip().str.len() > 0].copy()\n",
        "    after = len(df)\n",
        "    if after < before:\n",
        "        print(f\"• Dropped {before - after} empty-text rows\")\n",
        "    return df\n",
        "\n",
        "def build_global_label_mapping(train_df: pd.DataFrame, ood_df: pd.DataFrame) -> Dict[Any, int]:\n",
        "    \"\"\"\n",
        "    Build a consistent mapping over ALL labels in train + OOD so that\n",
        "    labels are contiguous ints 0..K-1. Return dict original_label -> idx.\n",
        "    \"\"\"\n",
        "    all_labels = pd.concat([train_df['label'], ood_df['label']], axis=0)\n",
        "    # If already int and contiguous 0..K-1, keep identity mapping\n",
        "    if pd.api.types.is_integer_dtype(all_labels):\n",
        "        uniq = sorted(all_labels.unique().tolist())\n",
        "        if uniq and uniq[0] == 0 and uniq[-1] == len(uniq)-1:\n",
        "            return {i: i for i in uniq}\n",
        "    # Otherwise, map sorted unique values to 0..K-1\n",
        "    uniq = sorted(all_labels.unique().tolist(), key=lambda x: str(x))\n",
        "    return {v: i for i, v in enumerate(uniq)}\n",
        "\n",
        "def apply_label_mapping(df: pd.DataFrame, mapping: Dict[Any, int]) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df['label'] = df['label'].map(mapping)\n",
        "    if df['label'].isna().any():\n",
        "        missing = df[df['label'].isna()]\n",
        "        raise ValueError(f\"Found labels not in mapping: {missing['label'].tolist()[:5]} ...\")\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    return df\n",
        "\n",
        "def summarize_dataset(train_df: pd.DataFrame, ood_df: pd.DataFrame):\n",
        "    print(\"\\n=== Dataset summary ===\")\n",
        "    print(f\"device: {DEVICE}\")\n",
        "    print(f\"train: {len(train_df):,} rows | ood: {len(ood_df):,} rows\")\n",
        "    # train label dist\n",
        "    tl = train_df['label'].value_counts().sort_index()\n",
        "    print(\"train label distribution:\", tl.to_dict())\n",
        "    if 'domain' in ood_df.columns:\n",
        "        dom_counts = ood_df['domain'].value_counts().to_dict()\n",
        "        print(\"ood domains:\", dom_counts)\n",
        "    print(\"=======================\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load, clean, map labels\n",
        "# -----------------------------\n",
        "seed_everything(13)\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# require CSVs on Drive\n",
        "_require_file(Config.TRAIN_PATH, \"train.csv\")\n",
        "_require_file(Config.OOD_DATA_PATH, \"ood_dataset.csv\")\n",
        "_require_file(Config.TERMS_PATH, \"engineering_terms.csv\")  # used later; just verifying now\n",
        "\n",
        "# read\n",
        "train_df = pd.read_csv(Config.TRAIN_PATH)\n",
        "ood_df   = pd.read_csv(Config.OOD_DATA_PATH)\n",
        "\n",
        "# basic column checks\n",
        "for c in ['content', 'label']:\n",
        "    if c not in train_df.columns:\n",
        "        raise KeyError(f\"train.csv missing required column: {c}\")\n",
        "for c in ['content', 'label', 'domain']:\n",
        "    if c not in ood_df.columns:\n",
        "        raise KeyError(f\"ood_dataset.csv missing required column: {c}\")\n",
        "\n",
        "# clean text; ensure lang; drop empty rows\n",
        "train_df = train_df.copy()\n",
        "train_df['content'] = _clean_text_series(train_df['content'])\n",
        "train_df = _ensure_lang_col(train_df)\n",
        "train_df = _drop_empty_rows(train_df, 'content')\n",
        "\n",
        "ood_df = ood_df.copy()\n",
        "ood_df['content'] = _clean_text_series(ood_df['content'])\n",
        "ood_df = _ensure_lang_col(ood_df)\n",
        "ood_df = _drop_empty_rows(ood_df, 'content')\n",
        "\n",
        "# build & apply a GLOBAL label mapping  (IMPORTANT for consistency)\n",
        "label_map = build_global_label_mapping(train_df, ood_df)\n",
        "with open(os.path.join(Config.RUN_DIR, \"label_mapping.json\"), \"w\") as f:\n",
        "    json.dump({str(k): int(v) for k, v in label_map.items()}, f, indent=2)\n",
        "\n",
        "train_df = apply_label_mapping(train_df, label_map)\n",
        "ood_df   = apply_label_mapping(ood_df,   label_map)\n",
        "\n",
        "# infer NUM_LABELS from mapped data and store into Config\n",
        "Config.NUM_LABELS = int(pd.unique(pd.concat([train_df['label'], ood_df['label']], axis=0)).shape[0])\n",
        "print(\"Detected NUM_LABELS =\", Config.NUM_LABELS)\n",
        "\n",
        "# (optional) persist cleaned copies used for all runs (for reproducibility)\n",
        "train_clean_path = os.path.join(Config.RUN_DIR, \"train_clean.csv\")\n",
        "ood_clean_path   = os.path.join(Config.RUN_DIR, \"ood_clean.csv\")\n",
        "train_df.to_csv(train_clean_path, index=False)\n",
        "ood_df.to_csv(ood_clean_path, index=False)\n",
        "print(f\"Saved cleaned datasets to:\\n  {train_clean_path}\\n  {ood_clean_path}\")\n",
        "\n",
        "# quick summary\n",
        "summarize_dataset(train_df, ood_df)\n",
        "\n",
        "# NOTE:\n",
        "# - Next cells (training, DAPT loading, fusion, adaptation, etc.) should\n",
        "#   reference paths from Config.* and use the already-loaded train_df/ood_df.\n",
        "# - All artifacts will be saved to /content/emc_run (RUN_DIR), keeping Drive clean.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FWz1DBgspEDw",
        "outputId": "7e3fa951-f8c7-43b6-8cda-51e30ad51d5d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Detected NUM_LABELS = 2\n",
            "Saved cleaned datasets to:\n",
            "  /content/emc_run/train_clean.csv\n",
            "  /content/emc_run/ood_clean.csv\n",
            "\n",
            "=== Dataset summary ===\n",
            "device: cuda\n",
            "train: 11,610 rows | ood: 600 rows\n",
            "train label distribution: {0: 8238, 1: 3372}\n",
            "ood domains: {'Biomedical Engineering': 200, 'Chemical Engineering': 200, 'Software Engineering': 200}\n",
            "=======================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block B — Tokenizer, features, datasets, fusion models & utils\n",
        "# ============================================\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "\n",
        "# --- Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
        "\n",
        "# --- Terms lexicon & 12D feature extractor -----------------\n",
        "import re\n",
        "\n",
        "_WORD_RE = re.compile(r\"\\w+\", re.UNICODE)\n",
        "def simple_words(t: str): return _WORD_RE.findall(t or \"\")\n",
        "\n",
        "def sent_count(t: str) -> int:\n",
        "    if not t: return 0\n",
        "    return max(1, len(re.split(r'[.!?]+[\\s\\n]+', t)))\n",
        "\n",
        "def punct_count(t: str) -> int:\n",
        "    return sum(1 for ch in (t or \"\") if ch in \".,;:!?\")\n",
        "\n",
        "class TermsLexicon:\n",
        "    def __init__(self, csv_path: str, term_col=\"terms\", lang_col=\"lang\"):\n",
        "        import pandas as pd, os\n",
        "        if not os.path.exists(csv_path): raise FileNotFoundError(csv_path)\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if term_col not in df.columns: raise ValueError(f\"Missing '{term_col}'\")\n",
        "        if lang_col not in df.columns: df[lang_col] = 'en'\n",
        "        self.by_lang = {\n",
        "            str(l).lower(): set(\n",
        "                str(x).strip().lower()\n",
        "                for x in d[term_col].dropna().tolist() if str(x).strip()\n",
        "            )\n",
        "            for l, d in df.groupby(lang_col)\n",
        "        }\n",
        "\n",
        "    def pct_in_text(self, text: str, lang: str) -> float:\n",
        "        if not text: return 0.0\n",
        "        terms = self.by_lang.get((lang or \"en\").lower(), set())\n",
        "        if not terms: return 0.0\n",
        "        ws = [w.lower() for w in simple_words(text)]\n",
        "        if not ws: return 0.0\n",
        "        return sum(1 for w in ws if w in terms) / max(1, len(ws))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "_NUM_RE = re.compile(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\")\n",
        "def extract_numbers(text: str):\n",
        "    nums, dec = [], 0\n",
        "    for m in _NUM_RE.finditer(text or \"\"):\n",
        "        s = m.group(0)\n",
        "        try:\n",
        "            v = float(s)\n",
        "            if ('.' in s) or ('e' in s.lower()): dec += 1\n",
        "            nums.append(abs(v))\n",
        "        except: pass\n",
        "    return nums, dec\n",
        "\n",
        "def _finite_or_zero(x: float) -> float:\n",
        "    return float(x) if np.isfinite(x) else 0.0\n",
        "\n",
        "# Optional textstat for English readability\n",
        "try:\n",
        "    import textstat\n",
        "    _HAS_TEXTSTAT = True\n",
        "except Exception:\n",
        "    _HAS_TEXTSTAT = False\n",
        "\n",
        "STD_TERMS = {\"iso\",\"asme\",\"ieee\",\"din\",\"ansi\",\"iec\",\"ul\",\"astm\",\"en\"}\n",
        "SAFETY_TERMS = {\"safety\",\"hazard\",\"warning\",\"risk\",\"caution\",\"danger\",\"emergency\"}\n",
        "\n",
        "class FeatureExtractor12:\n",
        "    MAX_CHARS, MAX_WORDS, MAX_SENTS, MAX_PUNCT, MAX_NUM_COUNT = 200_000, 40_000, 10_000, 50_000, 10_000\n",
        "    MAX_NUM_ABS, MAX_AVG_MAG = 1e12, 1e12\n",
        "    def __init__(self, tlex: TermsLexicon): self.tlex = tlex\n",
        "\n",
        "    def extract_one(self, text: str, lang: str) -> np.ndarray:\n",
        "        text = \"\" if text is None else str(text); lang = (lang or \"en\").lower()\n",
        "        ws = simple_words(text); n_words = len(ws)\n",
        "        chars = min(len(text), self.MAX_CHARS)\n",
        "        words = min(n_words, self.MAX_WORDS)\n",
        "        sents = min(sent_count(text), self.MAX_SENTS)\n",
        "\n",
        "        if lang == \"en\" and _HAS_TEXTSTAT and text.strip():\n",
        "            fre = _finite_or_zero(textstat.flesch_reading_ease(text))\n",
        "            fog = _finite_or_zero(textstat.gunning_fog(text))\n",
        "        else:\n",
        "            fre = fog = 0.0\n",
        "\n",
        "        eng_pct = self.tlex.pct_in_text(text, lang)\n",
        "        punc = min(punct_count(text), self.MAX_PUNCT)\n",
        "\n",
        "        nums, dec_cnt = extract_numbers(text)\n",
        "        nnums = min(len(nums), self.MAX_NUM_COUNT)\n",
        "        avg_mag = min(float(np.mean([min(v, self.MAX_NUM_ABS) for v in nums])) if nums else 0.0, self.MAX_AVG_MAG)\n",
        "        dec_ratio = float(dec_cnt / len(nums)) if nums else 0.0\n",
        "\n",
        "        low = text.lower()\n",
        "        has_std = 1.0 if any(t in low for t in STD_TERMS) else 0.0\n",
        "        has_saf = 1.0 if any(t in low for t in SAFETY_TERMS) else 0.0\n",
        "\n",
        "        feats = np.array([chars, words, sents, fre, fog, eng_pct, punc, nnums, has_std, has_saf, avg_mag, dec_ratio], dtype=np.float32)\n",
        "        if not np.all(np.isfinite(feats)):\n",
        "            feats = np.nan_to_num(feats, nan=0.0, posinf=self.MAX_AVG_MAG, neginf=0.0)\n",
        "        return feats.astype(np.float32)\n",
        "\n",
        "    def extract_df(self, df: pd.DataFrame) -> np.ndarray:\n",
        "        assert 'content' in df.columns\n",
        "        if 'lang' not in df.columns:\n",
        "            df = df.copy(); df['lang'] = 'en'\n",
        "        rows = [self.extract_one(r.get(\"content\",\"\"), r.get(\"lang\",\"en\")) for _, r in tqdm(df.iterrows(), total=len(df), desc=\"Extracting features\")]\n",
        "        return np.stack(rows, axis=0).astype(np.float32)\n",
        "\n",
        "tlex = TermsLexicon(Config.TERMS_PATH)\n",
        "fe = FeatureExtractor12(tlex)\n",
        "\n",
        "# --- Datasets ----------------------------------------------\n",
        "class TextFeatDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, tokenizer, feats: Optional[np.ndarray] = None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tok = tokenizer\n",
        "        self.feats = feats\n",
        "        self.labels = df['label'].values.astype(int) if 'label' in df.columns else None\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        enc = self.tok(\n",
        "            str(row['content']),\n",
        "            truncation=True,\n",
        "            max_length=Config.MAX_LEN,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {\n",
        "            'input_ids': enc['input_ids'].squeeze(0),\n",
        "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
        "        }\n",
        "        if self.feats is not None:\n",
        "            item['feats'] = torch.tensor(self.feats[idx], dtype=torch.float32)\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "def make_loader(dataset, batch_size, sampler=None, shuffle=False):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=sampler,\n",
        "        shuffle=(shuffle if sampler is None else False),\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=Config.PIN_MEMORY,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "# --- Fusion models ------------------------------------------\n",
        "def masked_mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "    mask = attention_mask.unsqueeze(-1).float()\n",
        "    return (last_hidden_state * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n",
        "\n",
        "class SimpleFusion(nn.Module):\n",
        "    def __init__(self, model_name: str, n_feats: int, n_labels: int = None):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        try: self.encoder.gradient_checkpointing_enable()\n",
        "        except Exception: pass\n",
        "        H = self.encoder.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(H + n_feats, n_labels or Config.NUM_LABELS)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, feats):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        pooled = masked_mean_pool(out.last_hidden_state, attention_mask)\n",
        "        fused = torch.cat([pooled, feats], dim=1)\n",
        "        return self.classifier(self.dropout(fused))\n",
        "\n",
        "class GatedFusion(nn.Module):\n",
        "    def __init__(self, model_name: str, n_feats: int, n_labels: int = None, feat_proj: int = 64):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        try: self.encoder.gradient_checkpointing_enable()\n",
        "        except Exception: pass\n",
        "        H = self.encoder.config.hidden_size\n",
        "        self.fe_proj = nn.Sequential(nn.Linear(n_feats, feat_proj), nn.ReLU())\n",
        "        self.gate = nn.Sequential(nn.Linear(H + n_feats, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(H + feat_proj, n_labels or Config.NUM_LABELS)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, feats):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        pooled = masked_mean_pool(out.last_hidden_state, attention_mask)\n",
        "        alpha = self.gate(torch.cat([pooled, feats], dim=1))\n",
        "        ef = self.fe_proj(feats)\n",
        "        fused = torch.cat([pooled, alpha * ef], dim=1)\n",
        "        return self.classifier(self.dropout(fused))\n",
        "\n",
        "def unfreeze_top_n(model: nn.Module, top_n: int):\n",
        "    # Works for HF encoders (roberta/xlm_roberta)\n",
        "    enc = (getattr(model, \"base_model\", None)\n",
        "           or getattr(model, \"roberta\", None)\n",
        "           or getattr(model, \"xlm_roberta\", None)\n",
        "           or getattr(model, \"encoder\", None)  # for fusion models' encoder\n",
        "          )\n",
        "    if enc is None:\n",
        "        return\n",
        "    if hasattr(enc, \"encoder\") and hasattr(enc.encoder, \"layer\"):\n",
        "        layers = enc.encoder.layer\n",
        "        K = len(layers)\n",
        "        for i, layer in enumerate(layers):\n",
        "            req = (i >= K - top_n)\n",
        "            for p in layer.parameters(): p.requires_grad = req\n",
        "    # always keep classifier trainable\n",
        "    if hasattr(model, \"classifier\"):\n",
        "        for p in model.classifier.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "print(\"Block B ready ✔️\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163,
          "referenced_widgets": [
            "65fa2de1589440ee848d6a99b07a43f3",
            "bcca5730a7c14b74844c7f8c186a90d2",
            "756b06e4dca34afc97b757fa8b2089be",
            "52b5969097884a94b769cc1b6f1cf45d",
            "167bcec9dad0429594d0fbee3d3c204b",
            "5c96fb7a074342fe8b85093909f6b636",
            "d969692dc4014f3e8c2f3e4f94ae0263",
            "3ae9cab0f4464ad0a715d72cbdd98187",
            "b865a055f1b841a59ff0ca500e99e87f",
            "d5978b54e1474cad983927e4f9c840cf",
            "c1db91a55ccc42249559e2ba984a783d",
            "0140df75139e4c438984f366cc59f1ad",
            "8dc3869641fa4a3ba6c4209d0208f9a5",
            "25a3fe0cfbbe4227be1261026116f082",
            "85b450a8479c4af9b9fab3b47fe18008",
            "91da408ecd11432cb0cdcd8d993e1326",
            "7593c8cab78e4c1b8d17ae5006b0ea43",
            "b623fc2f25ec4f5d9aed17b8022aacf4",
            "81ffc16020884158be2eb4332c550ad3",
            "1d5e4eb60662440a9baad1ef68dcd84c",
            "002088ab548d45cb875508a120abfa14",
            "3d532a3e5e594178a2b54b6aaa059de3",
            "7573ed97db194295ab41947973d68ba8",
            "6739fa7b87cc438b829282f4ce7017e5",
            "404f9c92a5124cbc9da4de5a1913dad6",
            "da34c9883cb84daeb7eb9c04d53ca25d",
            "ea9edc61451d449f8db48fa0307fcbed",
            "de4b8884f1bf443f98a5f6b416d708ce",
            "af6d307034ff45f89569b818fb13b5ab",
            "a703924972d74f1a98928137a3e3652c",
            "4284e864c02f4b7dba3c246d183366b3",
            "6eb9fd86dd2045b0adb44ba582c3490f",
            "4cc84a329e064850b872457ebc480da1",
            "dbda1239d57c44d8a5ee46e4d0c35c81",
            "34fdad5d01314b3a8b6916f48b590cbb",
            "6d9f4a8d65304d1393138d39fb7ed3f6",
            "0d9fbca6f5a8444e8ee686e0487902a2",
            "665520c2a9a74a73baaea1fcfe9fc038",
            "b46c973367834ee79df57f5c0a33f3e2",
            "48447e9716144be5a24d3dc06cf57a47",
            "e32acfd2921e456f9e006f81c993d414",
            "6cc74bdb28ed411782072c34c68d1da0",
            "55b29e335d6e47339c4f2aa7fa263ae0",
            "f94101a886e0486394f8c54fc60d0b9c"
          ]
        },
        "id": "NofsAp7Bpgyt",
        "outputId": "40b82444-4862-47a6-f9eb-cf51f3ca201c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65fa2de1589440ee848d6a99b07a43f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0140df75139e4c438984f366cc59f1ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7573ed97db194295ab41947973d68ba8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbda1239d57c44d8a5ee46e4d0c35c81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Block B ready ✔️\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block C — DAPT training + DAPT/Head loaders (robust copy)\n",
        "# ============================================\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForMaskedLM, AutoModelForSequenceClassification,\n",
        "    DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        ")\n",
        "\n",
        "# --- Unlabeled dataset for MLM\n",
        "class UnlabeledTextDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], tokenizer, max_len: int):\n",
        "        self.texts = texts\n",
        "        self.tok = tokenizer\n",
        "        self.max_len = max_len\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tok(\n",
        "            str(self.texts[idx]),\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {'input_ids': enc['input_ids'].squeeze(0), 'attention_mask': enc['attention_mask'].squeeze(0)}\n",
        "\n",
        "def run_mlm_corpus_training(\n",
        "    texts: List[str],\n",
        "    save_dir: str,\n",
        "    epochs: int,\n",
        "    batch: int,\n",
        "    lr: float,\n",
        "    warmup_ratio: float,\n",
        "    max_steps: Optional[int],\n",
        "    tokenizer,\n",
        "):\n",
        "    if len(texts) == 0:\n",
        "        print(\"⚠️ DAPT skipped: no texts\")\n",
        "        return\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model = AutoModelForMaskedLM.from_pretrained(Config.MODEL_NAME)\n",
        "    model.to(DEVICE)\n",
        "    # ✅ use shorter seq length for MLM to be memory-safe\n",
        "    dataset = UnlabeledTextDataset(texts, tokenizer, max_len=Config.MLM_MAX_LEN)\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=save_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch,\n",
        "        learning_rate=lr,\n",
        "        warmup_ratio=warmup_ratio,\n",
        "        logging_steps=50,\n",
        "        save_strategy=\"no\",   # keep only final weights we save manually\n",
        "        fp16=(DEVICE==\"cuda\"),\n",
        "        gradient_checkpointing=True,\n",
        "        max_steps=max_steps,\n",
        "        dataloader_num_workers=Config.NUM_WORKERS,\n",
        "        report_to=\"none\",\n",
        "        disable_tqdm=True,\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    print(f\"Starting DAPT MLM on {DEVICE} …\")\n",
        "    trainer.train()\n",
        "    # Write a plain state_dict() to a simple bin path\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
        "    print(\"DAPT complete.\")\n",
        "\n",
        "# ---- Helper: materialize the MLM model then copy its encoder 1:1\n",
        "def _materialize_mlm_encoder_state(bin_path: str, base_model_name: str):\n",
        "    \"\"\"\n",
        "    Loads a base MLM model, applies the saved state_dict (whatever its prefixes),\n",
        "    and returns *its encoder* state_dict. This sidesteps key-prefix mismatches.\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(bin_path):\n",
        "        return None\n",
        "    # Create a fresh MLM model with the same architecture\n",
        "    mlm = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
        "    sd = torch.load(bin_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"state_dict\" in sd:\n",
        "        sd = sd[\"state_dict\"]\n",
        "    # Strip a leading \"model.\" if present\n",
        "    if sd and all(k.startswith(\"model.\") for k in sd.keys()):\n",
        "        sd = {k[len(\"model.\"):]: v for k, v in sd.items()}\n",
        "    # Load what we can (strict=False is intentional)\n",
        "    mlm.load_state_dict(sd, strict=False)\n",
        "\n",
        "    src_enc = (getattr(mlm, \"roberta\", None)\n",
        "               or getattr(mlm, \"xlm_roberta\", None))\n",
        "    if src_enc is None:\n",
        "        return None\n",
        "    return src_enc.state_dict()\n",
        "\n",
        "def safe_load_seqcls_from_dapt(dapt_dir: str, num_labels: int, base_model_name: str):\n",
        "    \"\"\"\n",
        "    Build a SequenceClassification model and copy ONLY encoder weights from\n",
        "    the DAPT MLM checkpoint by *materializing the MLM encoder* first.\n",
        "    \"\"\"\n",
        "    m = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=num_labels)\n",
        "    try: m.gradient_checkpointing_enable()\n",
        "    except Exception: pass\n",
        "\n",
        "    bin_path = os.path.join(dapt_dir, \"pytorch_model.bin\")\n",
        "    src_enc_sd = _materialize_mlm_encoder_state(bin_path, base_model_name)\n",
        "    if src_enc_sd is None:\n",
        "        print(f\"⚠️ No usable DAPT bin at {bin_path}; using base encoder.\")\n",
        "        return m\n",
        "\n",
        "    dest_enc = (getattr(m, \"base_model\", None)\n",
        "                or getattr(m, \"roberta\", None)\n",
        "                or getattr(m, \"xlm_roberta\", None))\n",
        "    if dest_enc is None:\n",
        "        print(\"⚠️ Could not locate encoder in classifier; skipping DAPT copy.\")\n",
        "        return m\n",
        "\n",
        "    dest_sd = dest_enc.state_dict()\n",
        "    copied = 0\n",
        "    new_sd = {}\n",
        "    for k in dest_sd.keys():\n",
        "        if k in src_enc_sd:\n",
        "            new_sd[k] = src_enc_sd[k]\n",
        "            copied += 1\n",
        "    dest_enc.load_state_dict({**dest_sd, **new_sd}, strict=False)\n",
        "    print(f\"✅ DAPT encoder load (robust): copied {copied}/{len(dest_sd)} tensors from {bin_path}\")\n",
        "    return m\n",
        "\n",
        "def load_ft_classifier_only_into(model, ft_state_path: str):\n",
        "    \"\"\"Load ONLY the classifier head from an in-domain fine-tuned checkpoint.\"\"\"\n",
        "    if not os.path.isfile(ft_state_path):\n",
        "        print(f\"⚠️ Classifier ckpt not found at {ft_state_path}\")\n",
        "        return\n",
        "    sd = torch.load(ft_state_path, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"state_dict\" in sd:\n",
        "        sd = sd[\"state_dict\"]\n",
        "    head = {k: v for k, v in sd.items() if k.startswith(\"classifier.\")}\n",
        "    missing, unexpected = model.load_state_dict(head, strict=False)\n",
        "    print(f\"✅ Loaded classifier head from {ft_state_path} (kept encoder). \"\n",
        "          f\"missing={len(missing)} unexpected={len(unexpected)}\")\n",
        "\n",
        "def fusion_available(pt_path: str, scaler_path: str) -> bool:\n",
        "    return os.path.isfile(pt_path) and os.path.isfile(scaler_path)\n",
        "\n",
        "def load_dapt_into_fusion_encoder(fusion_model, dapt_dir: str, base_model_name: str):\n",
        "    \"\"\"Copy DAPT encoder weights into fusion.encoder using the robust source encoder trick.\"\"\"\n",
        "    bin_path = os.path.join(dapt_dir, \"pytorch_model.bin\")\n",
        "    src_enc_sd = _materialize_mlm_encoder_state(bin_path, base_model_name)\n",
        "    if src_enc_sd is None:\n",
        "        print(f\"⚠️ No usable DAPT bin at {bin_path}; skipping DAPT→fusion.\")\n",
        "        return\n",
        "    enc = getattr(fusion_model, \"encoder\", None)\n",
        "    if enc is None:\n",
        "        print(\"⚠️ Fusion has no .encoder; skip.\")\n",
        "        return\n",
        "    dest_sd = enc.state_dict()\n",
        "    copied = 0\n",
        "    new_sd = {}\n",
        "    for k in dest_sd.keys():\n",
        "        if k in src_enc_sd:\n",
        "            new_sd[k] = src_enc_sd[k]\n",
        "            copied += 1\n",
        "    enc.load_state_dict({**dest_sd, **new_sd}, strict=False)\n",
        "    print(f\"✅ DAPT→Fusion encoder (robust): copied {copied}/{len(dest_sd)} tensors from {bin_path}\")\n",
        "\n",
        "print(\"Block C ready ✔️\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_t6zJBDqJCr",
        "outputId": "f1bcc15e-b5af-4bf7-877f-d6d9bd43f828"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Block C ready ✔️\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block D — Supervised fine-tuning (base classifier)\n",
        "# ============================================\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_transformer_supervised(train_df: pd.DataFrame, tokenizer, num_labels: int):\n",
        "    tr, va = train_test_split(train_df, test_size=Config.VAL_SPLIT, stratify=train_df['label'], random_state=13)\n",
        "    ds_tr = TextFeatDataset(tr, tokenizer, feats=None)\n",
        "    ds_va = TextFeatDataset(va, tokenizer, feats=None)\n",
        "    dl_tr = make_loader(ds_tr, batch_size=32, shuffle=True)\n",
        "    dl_va = make_loader(ds_va, batch_size=64)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(Config.MODEL_NAME, num_labels=num_labels).to(DEVICE)\n",
        "    try: model.gradient_checkpointing_enable()\n",
        "    except Exception: pass\n",
        "\n",
        "    # Freeze embeddings + bottom layers for stability\n",
        "    enc = (getattr(model, \"base_model\", None)\n",
        "           or getattr(model, \"roberta\", None)\n",
        "           or getattr(model, \"xlm_roberta\", None))\n",
        "    if enc is not None and hasattr(enc, \"embeddings\"):\n",
        "        for p in enc.embeddings.parameters(): p.requires_grad = False\n",
        "    if enc is not None and hasattr(enc, \"encoder\") and hasattr(enc.encoder, \"layer\"):\n",
        "        for i, layer in enumerate(enc.encoder.layer):\n",
        "            if i < 4:  # freeze bottom 4\n",
        "                for p in layer.parameters(): p.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, weight_decay=0.01)\n",
        "    total_steps = len(dl_tr) * 3\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, int(0.06*total_steps), total_steps)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    best_f1, bad = -1.0, 0\n",
        "    for epoch in range(3):\n",
        "        model.train(); running = 0.0\n",
        "        for b in tqdm(dl_tr, desc=f\"FT Epoch {epoch+1}/3\", leave=False):\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); y=b['labels'].to(DEVICE)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                out = model(input_ids=ids, attention_mask=am, return_dict=True)\n",
        "                loss = loss_fn(out.logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer); scaler.update(); scheduler.step()\n",
        "            running += float(loss.item())\n",
        "\n",
        "        # val\n",
        "        f1 = evaluate_model(model, dl_va, 'transformer')\n",
        "        print(f\"Epoch {epoch+1}: loss={running/max(1,len(dl_tr)):.4f}  val_macroF1={f1:.4f}\")\n",
        "        if f1 > best_f1:\n",
        "            best_f1, bad = f1, 0\n",
        "            os.makedirs(os.path.dirname(Config.XLM_R_MODEL_PATH), exist_ok=True)\n",
        "            torch.save(model.state_dict(), Config.XLM_R_MODEL_PATH)\n",
        "            print(\"✅ Saved best model to RUN_DIR.\")\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= 2:\n",
        "                print(\"⏹️ Early stopping\")\n",
        "                break\n",
        "\n",
        "print(\"Block D ready ✔️\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs4AdEWcqN2_",
        "outputId": "b6fc25ca-0219-4c5a-909d-f4902c9da2b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Block D ready ✔️\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block E — Eval/proba/uncertainty + adaptation\n",
        "# ============================================\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def evaluate_model(model, data_loader, model_type: str = 'transformer') -> float:\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=f\"Evaluating {model_type}\", leave=False):\n",
        "            ids = batch['input_ids'].to(DEVICE); am = batch['attention_mask'].to(DEVICE)\n",
        "            if 'labels' in batch:\n",
        "                all_labels.extend(batch['labels'].cpu().numpy().tolist())\n",
        "\n",
        "            if model_type == 'transformer':\n",
        "                with autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                    logits = model(input_ids=ids, attention_mask=am, return_dict=True).logits\n",
        "            elif model_type in ('simple','gated'):\n",
        "                feats = batch['feats'].to(DEVICE)\n",
        "                logits = model(ids, am, feats)\n",
        "            else:\n",
        "                raise ValueError(model_type)\n",
        "\n",
        "            preds = logits.argmax(1).cpu().tolist()\n",
        "            all_preds.extend(preds)\n",
        "    return f1_score(all_labels, all_preds, average='macro') if all_labels else float(\"nan\")\n",
        "\n",
        "def get_probs_transformer(model, ds, idx, batch=64):\n",
        "    if len(idx) == 0: return np.array([])\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out = []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE)\n",
        "            with autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                logits = model(input_ids=ids, attention_mask=am, return_dict=True).logits\n",
        "            p = torch.softmax(logits, dim=-1)[:, 1].cpu().numpy()\n",
        "            out.append(p)\n",
        "    return np.concatenate(out) if out else np.array([])\n",
        "\n",
        "def get_probs_fusion(model, ds, idx, batch=64):\n",
        "    if len(idx)==0: return np.array([])\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out=[]\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); feats=b['feats'].to(DEVICE)\n",
        "            logits = model(ids, am, feats)\n",
        "            p = torch.softmax(logits, dim=-1)[:, 1].cpu().numpy()\n",
        "            out.append(p)\n",
        "    return np.concatenate(out) if out else np.array([])\n",
        "\n",
        "def pick_uncertain_indices_with_proba(proba_fn, k, n):\n",
        "    idx_all = np.arange(n)\n",
        "    if k <= 0 or n <= k:\n",
        "        return np.array([], dtype=int), idx_all\n",
        "    probs = proba_fn(idx_all)\n",
        "    if probs.size == 0:\n",
        "        rng = np.random.default_rng(13); rng.shuffle(idx_all)\n",
        "        return idx_all[:k], idx_all[k:]\n",
        "    p = np.clip(probs, 1e-6, 1-1e-6)\n",
        "    ent = -(p*np.log(p) + (1-p)*np.log(1-p))\n",
        "    order = np.argsort(-ent)\n",
        "    adapt_idx = idx_all[order[:k]]\n",
        "    test_idx  = idx_all[order[k:]]\n",
        "    return adapt_idx, test_idx\n",
        "\n",
        "def tune_weight_and_threshold(p_t, p_x, y_true, weights=None, thresholds=None):\n",
        "    if len(p_t)==0 or len(y_true)==0:\n",
        "        return (-1.0, 1.0, 0.5)\n",
        "    if weights is None: weights = np.linspace(0.0, 1.0, 11)\n",
        "    if thresholds is None: thresholds = np.linspace(0.2, 0.8, 25)\n",
        "    best = (-1.0, 1.0, 0.5)\n",
        "    for w in weights:\n",
        "        mix = w * p_t + (1 - w) * (p_x if p_x is not None and len(p_x)==len(p_t) else 0.0)\n",
        "        for thr in thresholds:\n",
        "            preds = (mix >= thr).astype(int)\n",
        "            f1 = f1_score(y_true, preds, average='macro')\n",
        "            if f1 > best[0]: best = (f1, w, thr)\n",
        "    return best\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def adapt_transformer_fewshot(model, ds_all, adapt_idx, lr, epochs):\n",
        "    dl = make_loader(ds_all, batch_size=Config.BATCH_SIZE, sampler=SubsetRandomSampler(adapt_idx))\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    steps = max(1, len(dl)*epochs)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, int(0.06*steps), steps)\n",
        "    scaler_amp = GradScaler()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); y=b['labels'].to(DEVICE)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                logits = model(input_ids=ids, attention_mask=am, return_dict=True).logits\n",
        "                loss = loss_fn(logits, y)\n",
        "            scaler_amp.scale(loss).backward()\n",
        "            scaler_amp.step(optimizer); scaler_amp.update(); scheduler.step()\n",
        "\n",
        "def adapt_fusion_fewshot(model, ds_all, adapt_idx, lr, epochs):\n",
        "    dl = make_loader(ds_all, batch_size=Config.BATCH_SIZE, sampler=SubsetRandomSampler(adapt_idx))\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    scaler_amp = GradScaler()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); y=b['labels'].to(DEVICE); feats=b['feats'].to(DEVICE)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                logits = model(ids, am, feats)\n",
        "                loss = loss_fn(logits, y)\n",
        "            scaler_amp.scale(loss).backward()\n",
        "            scaler_amp.step(optimizer); scaler_amp.update()\n",
        "\n",
        "print(\"Block E ready ✔️\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VJQaAtGqQ70",
        "outputId": "8894d76d-91dc-4621-9437-7197c013a4f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Block E ready ✔️\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block F — Fit scalers on train, init XGB + Fusion (no leakage)\n",
        "# ============================================\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"Extracting features for in-domain training (XGB & fusion scalers)…\")\n",
        "feats_tr_raw = fe.extract_df(train_df)\n",
        "y_tr = train_df['label'].astype(int).values\n",
        "\n",
        "# XGB scaler (fit on train only)\n",
        "xgb_scaler = StandardScaler()\n",
        "feats_tr_scaled = xgb_scaler.fit_transform(feats_tr_raw)\n",
        "\n",
        "# XGB baseline\n",
        "try:\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        n_estimators=1500,\n",
        "        objective=\"binary:logistic\" if Config.NUM_LABELS == 2 else \"multi:softprob\",\n",
        "        num_class=(Config.NUM_LABELS if Config.NUM_LABELS>2 else None),\n",
        "        tree_method=\"hist\", n_jobs=4, random_state=13,\n",
        "        eval_metric=\"logloss\" if Config.NUM_LABELS==2 else \"mlogloss\"\n",
        "    )\n",
        "    xgb_model.fit(feats_tr_scaled, y_tr)\n",
        "    print(\"XGBoost baseline trained.\")\n",
        "except Exception as e:\n",
        "    xgb_model = None\n",
        "    print(\"⚠️ xgboost not installed or failed; skipping XGB baseline.\", e)\n",
        "\n",
        "# Fusion scalers (fit on train only; persist in RUN_DIR)\n",
        "os.makedirs(os.path.dirname(Config.SIMPLE_SCALER_PATH), exist_ok=True)\n",
        "os.makedirs(os.path.dirname(Config.GATED_SCALER_PATH), exist_ok=True)\n",
        "\n",
        "if os.path.isfile(Config.SIMPLE_SCALER_PATH):\n",
        "    simple_scaler = joblib.load(Config.SIMPLE_SCALER_PATH)\n",
        "else:\n",
        "    simple_scaler = StandardScaler().fit(feats_tr_raw)\n",
        "    joblib.dump(simple_scaler, Config.SIMPLE_SCALER_PATH)\n",
        "\n",
        "if os.path.isfile(Config.GATED_SCALER_PATH):\n",
        "    gated_scaler = joblib.load(Config.GATED_SCALER_PATH)\n",
        "else:\n",
        "    gated_scaler = StandardScaler().fit(feats_tr_raw)\n",
        "    joblib.dump(gated_scaler, Config.GATED_SCALER_PATH)\n",
        "\n",
        "# Init fusion models; load any saved heads if present\n",
        "simple = SimpleFusion(Config.MODEL_NAME, Config.FEAT_DIM, n_labels=Config.NUM_LABELS).to(DEVICE)\n",
        "gated  = GatedFusion(Config.MODEL_NAME, Config.FEAT_DIM, n_labels=Config.NUM_LABELS, feat_proj=64).to(DEVICE)\n",
        "\n",
        "if os.path.isfile(Config.SIMPLE_PT_PATH):\n",
        "    sd = torch.load(Config.SIMPLE_PT_PATH, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n",
        "    simple.load_state_dict(sd, strict=False)\n",
        "\n",
        "if os.path.isfile(Config.GATED_PT_PATH):\n",
        "    sd = torch.load(Config.GATED_PT_PATH, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n",
        "    gated.load_state_dict(sd, strict=False)\n",
        "\n",
        "print(\"Block F ready ✔️\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "fdd6c46d40a24a0fabedca74070a44c1",
            "863600cb9a624380a15bbe8250c18c4c",
            "227b48f09ee343febdde8b899c8ec9fa",
            "78a0d8e6d2754208ad3e621405adac56",
            "7fa52b6a479b4c95b840bf770e2ce8dc",
            "2d2ed8d274fa4aae8a6f923c4c1993ea",
            "9b673ed14e5840559ff4870bf9851050",
            "ecc2593a3e8f43829575b5fbce9fab74",
            "53f7f837fa7241fa9a0f41965e207844",
            "6bd503285e554374b51bc8a7c6c604d6",
            "7bf9cea8e8c24bd7b8e720662e39b73a"
          ]
        },
        "id": "GOVfcg5JqUWg",
        "outputId": "16abc7d6-d991-4dca-82f4-1563fc55c266"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features for in-domain training (XGB & fusion scalers)…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 11610/11610 [00:22<00:00, 524.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost baseline trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdd6c46d40a24a0fabedca74070a44c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Block F ready ✔️\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block G — FT if needed, per-domain DAPT + adaptation loop, save\n",
        "# ============================================\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 1) Supervised FT (train once if no checkpoint)\n",
        "if not os.path.isfile(Config.XLM_R_MODEL_PATH):\n",
        "    print(\"🧪 Training base classifier (supervised FT)…\")\n",
        "    train_transformer_supervised(train_df, tokenizer, num_labels=Config.NUM_LABELS)\n",
        "else:\n",
        "    print(\"ℹ️ Found existing fine-tuned checkpoint; skipping FT.\")\n",
        "\n",
        "results: List[Dict[str, Any]] = []\n",
        "domains = ood_df['domain'].unique()\n",
        "\n",
        "for domain in domains:\n",
        "    print(f\"\\n--- Processing domain: {domain} ---\")\n",
        "    seed_everything(13)                    # reproducibility per domain\n",
        "    torch.cuda.empty_cache()               # avoid memory fragmentation\n",
        "\n",
        "    ddf = ood_df[ood_df['domain']==domain].copy().reset_index(drop=True)\n",
        "    y_true = ddf['label'].astype(int).values\n",
        "\n",
        "    # DAPT: train per-domain if missing\n",
        "    dapt_dir = os.path.join(Config.DAPT_SAVE_DIR, domain.replace(\" \", \"_\"))\n",
        "    if not os.path.isfile(os.path.join(dapt_dir, \"pytorch_model.bin\")):\n",
        "        print(f\"🧪 Running DAPT (MLM) for domain '{domain}' …\")\n",
        "        run_mlm_corpus_training(\n",
        "            ddf['content'].astype(str).tolist(),\n",
        "            save_dir=dapt_dir,\n",
        "            epochs=Config.DAPT_EPOCHS,\n",
        "            batch=Config.DAPT_BATCH,\n",
        "            lr=Config.DAPT_LR,\n",
        "            warmup_ratio=Config.DAPT_WARMUP_RATIO,\n",
        "            max_steps=Config.DAPT_MAX_STEPS,\n",
        "            tokenizer=tokenizer,\n",
        "        )\n",
        "\n",
        "    # Build DAPT+head classifier (robust encoder copy)\n",
        "    xlm_r = safe_load_seqcls_from_dapt(\n",
        "        dapt_dir=dapt_dir,\n",
        "        num_labels=Config.NUM_LABELS,\n",
        "        base_model_name=Config.MODEL_NAME\n",
        "    )\n",
        "    load_ft_classifier_only_into(xlm_r, Config.XLM_R_MODEL_PATH)\n",
        "    xlm_r.to(DEVICE)\n",
        "\n",
        "    # Push DAPT into fusion encoders (robust)\n",
        "    load_dapt_into_fusion_encoder(simple, dapt_dir, Config.MODEL_NAME)\n",
        "    load_dapt_into_fusion_encoder(gated,  dapt_dir, Config.MODEL_NAME)\n",
        "\n",
        "    # Build OOD datasets\n",
        "    feats_ood_raw = fe.extract_df(ddf)\n",
        "    ds_all        = TextFeatDataset(ddf, tokenizer, feats=None)\n",
        "    ds_simple_all = TextFeatDataset(ddf, tokenizer, feats=simple_scaler.transform(feats_ood_raw))\n",
        "    ds_gated_all  = TextFeatDataset(ddf, tokenizer, feats=gated_scaler.transform(feats_ood_raw))\n",
        "\n",
        "    n = len(ddf)\n",
        "    k_target = max(int(Config.DA_ADAPT_RATIO * n), 64)\n",
        "    k = min(k_target, max(0, n//2))\n",
        "\n",
        "    # ✅ Select *separate* uncertain sets per model (better adaptation for each)\n",
        "    proba_t = lambda idxs: get_probs_transformer(xlm_r, ds_all, idxs, batch=Config.BATCH_SIZE)\n",
        "    adapt_t, test_t = pick_uncertain_indices_with_proba(proba_t, k=k, n=n)\n",
        "    print(f\"[Transformer] Adapt: {len(adapt_t)} | Test: {len(test_t)}\")\n",
        "\n",
        "    proba_s = lambda idxs: get_probs_fusion(simple, ds_simple_all, idxs, batch=Config.BATCH_SIZE)\n",
        "    adapt_s, test_s = pick_uncertain_indices_with_proba(proba_s, k=k, n=n)\n",
        "    print(f\"[SimpleFusion] Adapt: {len(adapt_s)} | Test: {len(test_s)}\")\n",
        "\n",
        "    proba_g = lambda idxs: get_probs_fusion(gated, ds_gated_all, idxs, batch=Config.BATCH_SIZE)\n",
        "    adapt_g, test_g = pick_uncertain_indices_with_proba(proba_g, k=k, n=n)\n",
        "    print(f\"[GatedFusion] Adapt: {len(adapt_g)} | Test: {len(test_g)}\")\n",
        "\n",
        "    # ---- Zero-shot: transformer\n",
        "    f1 = evaluate_model(xlm_r, make_loader(ds_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(np.arange(n))), 'transformer')\n",
        "    results.append({'Domain':domain,'Model':'XLM-R Only','Evaluation':'Zero-Shot','Macro F1-Score':float(f1)})\n",
        "\n",
        "    # ---- Zero-shot: XGB\n",
        "    if 'xgb_model' in globals() and xgb_model is not None:\n",
        "        preds = xgb_model.predict(xgb_scaler.transform(feats_ood_raw))\n",
        "        f1 = f1_score(y_true, preds, average='macro')\n",
        "        results.append({'Domain':domain,'Model':'XGBoost + Features','Evaluation':'Zero-Shot','Macro F1-Score':float(f1)})\n",
        "\n",
        "    # ---- Zero-shot: fusion (DAPT encoders)\n",
        "    f1 = evaluate_model(simple, make_loader(ds_simple_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(np.arange(n))), 'simple')\n",
        "    results.append({'Domain':domain,'Model':'Simple Fusion','Evaluation':'Zero-Shot (DAPT encoder)','Macro F1-Score':float(f1)})\n",
        "\n",
        "    f1 = evaluate_model(gated, make_loader(ds_gated_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(np.arange(n))), 'gated')\n",
        "    results.append({'Domain':domain,'Model':'Gated Fusion','Evaluation':'Zero-Shot (DAPT encoder)','Macro F1-Score':float(f1)})\n",
        "\n",
        "    # ---- Zero-shot Ensemble tuned on transformer-adapt split (consistent)\n",
        "    if 'xgb_model' in globals() and xgb_model is not None and len(adapt_t)>0 and len(test_t)>0:\n",
        "        p_t_adapt = get_probs_transformer(xlm_r, ds_all, adapt_t, batch=Config.BATCH_SIZE)\n",
        "        p_x_adapt = xgb_model.predict_proba(xgb_scaler.transform(feats_ood_raw[adapt_t]))[:,1]\n",
        "        best_f1, best_w, best_thr = tune_weight_and_threshold(p_t_adapt, p_x_adapt, y_true[adapt_t])\n",
        "\n",
        "        p_t_test = get_probs_transformer(xlm_r, ds_all, test_t, batch=Config.BATCH_SIZE)\n",
        "        p_x_test = xgb_model.predict_proba(xgb_scaler.transform(feats_ood_raw[test_t]))[:,1]\n",
        "        p_blend = best_w*p_t_test + (1-best_w)*p_x_test\n",
        "        preds = (p_blend >= best_thr).astype(int)\n",
        "        f1_blend = f1_score(y_true[test_t], preds, average='macro')\n",
        "        results.append({'Domain':domain,'Model':f'Ensemble(T+XGB) w={best_w:.2f} thr={best_thr:.2f}',\n",
        "                        'Evaluation':'Zero-Shot (tuned on adapt)','Macro F1-Score':float(f1_blend)})\n",
        "\n",
        "    # ---- Domain Adaptation: transformer + fusion (each on its own adapt set)\n",
        "    if len(adapt_t)>0 and len(test_t)>0:\n",
        "        print(\"  Running Domain Adaptation…\")\n",
        "        unfreeze_top_n(xlm_r, Config.DA_UNFREEZE_TOP_N)\n",
        "        adapt_transformer_fewshot(xlm_r, ds_all, adapt_t, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "        f1 = evaluate_model(xlm_r, make_loader(ds_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(test_t)), 'transformer')\n",
        "        results.append({'Domain':domain,'Model':'XLM-R Only','Evaluation':'Domain Adapted','Macro F1-Score':float(f1)})\n",
        "\n",
        "    if len(adapt_s)>0 and len(test_s)>0:\n",
        "        unfreeze_top_n(simple, Config.DA_UNFREEZE_TOP_N)\n",
        "        adapt_fusion_fewshot(simple, ds_simple_all, adapt_s, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "        f1 = evaluate_model(simple, make_loader(ds_simple_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(test_s)), 'simple')\n",
        "        results.append({'Domain':domain,'Model':'Simple Fusion','Evaluation':'Domain Adapted','Macro F1-Score':float(f1)})\n",
        "\n",
        "    if len(adapt_g)>0 and len(test_g)>0:\n",
        "        unfreeze_top_n(gated, Config.DA_UNFREEZE_TOP_N)\n",
        "        adapt_fusion_fewshot(gated, ds_gated_all, adapt_g, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "        f1 = evaluate_model(gated, make_loader(ds_gated_all, Config.BATCH_SIZE, sampler=SubsetRandomSampler(test_g)), 'gated')\n",
        "        results.append({'Domain':domain,'Model':'Gated Fusion','Evaluation':'Domain Adapted','Macro F1-Score':float(f1)})\n",
        "\n",
        "    # Optional: ensemble after adaptation (still using transformer split for comparability)\n",
        "    if 'xgb_model' in globals() and xgb_model is not None and len(adapt_t)>0 and len(test_t)>0:\n",
        "        p_t_adapt2 = get_probs_transformer(xlm_r, ds_all, adapt_t, batch=Config.BATCH_SIZE)\n",
        "        p_x_adapt2 = xgb_model.predict_proba(xgb_scaler.transform(feats_ood_raw[adapt_t]))[:,1]\n",
        "        best_f1, best_w, best_thr = tune_weight_and_threshold(p_t_adapt2, p_x_adapt2, y_true[adapt_t])\n",
        "\n",
        "        p_t_test2 = get_probs_transformer(xlm_r, ds_all, test_t, batch=Config.BATCH_SIZE)\n",
        "        p_x_test2 = xgb_model.predict_proba(xgb_scaler.transform(feats_ood_raw[test_t]))[:,1]\n",
        "        p_blend2 = best_w*p_t_test2 + (1-best_w)*p_x_test2\n",
        "        preds2 = (p_blend2 >= best_thr).astype(int)\n",
        "        f1_blend2 = f1_score(y_true[test_t], preds2, average='macro')\n",
        "        results.append({'Domain':domain,'Model':f'Ensemble(T+XGB) w={best_w:.2f} thr={best_thr:.2f}',\n",
        "                        'Evaluation':'Domain Adapted (tuned)','Macro F1-Score':float(f1_blend2)})\n",
        "\n",
        "# Save & pretty print\n",
        "results_df = pd.DataFrame(results)\n",
        "os.makedirs(os.path.dirname(Config.RESULTS_CSV_PATH), exist_ok=True)\n",
        "results_df.to_csv(Config.RESULTS_CSV_PATH, index=False)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"CROSS-DOMAIN GENERALIZATION FINAL RESULTS\")\n",
        "print(\"=\"*80)\n",
        "from pandas import option_context\n",
        "with option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(nice_pivot(results_df))\n",
        "print(f\"\\n✅ Results summary saved to: {Config.RESULTS_CSV_PATH}\")\n",
        "\n",
        "print(\"Block G done ✔️\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "collapsed": true,
        "id": "V92mnWbtqeyD",
        "outputId": "4197c2d6-dc4d-4aa1-8d57-3765ad1cc050"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Training base classifier (supervised FT)…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFT Epoch 1/3:   0%|          | 0/327 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss=0.5897  val_macroF1=0.4151\n",
            "✅ Saved best model to RUN_DIR.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 2/3:   0%|          | 0/327 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: loss=0.5186  val_macroF1=0.4151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFT Epoch 3/3:   0%|          | 0/327 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: loss=0.4928  val_macroF1=0.4151\n",
            "⏹️ Early stopping\n",
            "\n",
            "--- Processing domain: Biomedical Engineering ---\n",
            "🧪 Running DAPT (MLM) for domain 'Biomedical Engineering' …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'Config' has no attribute 'DAPT_EPOCHS'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3411814688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mddf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdapt_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDAPT_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDAPT_BATCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDAPT_LR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'Config' has no attribute 'DAPT_EPOCHS'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Sanity Checks v2: fixed splits + class weighting + threshold calibration\n",
        "# Run AFTER Blocks A–G (where models, configs, datasets are defined)\n",
        "# ============================================\n",
        "\n",
        "import numpy as np, pandas as pd, os, torch\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------- Utilities from the pipeline (light wrappers) ----------\n",
        "def _predict_argmax_transformer(model, ds, idx, batch=64):\n",
        "    if len(idx) == 0: return np.array([], dtype=int)\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out=[]\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE)\n",
        "            logits = model(input_ids=ids, attention_mask=am, return_dict=True).logits\n",
        "            out.append(logits.argmax(1).cpu().numpy())\n",
        "    return np.concatenate(out) if out else np.array([], dtype=int)\n",
        "\n",
        "def _predict_argmax_fusion(model, ds, idx, batch=64):\n",
        "    if len(idx) == 0: return np.array([], dtype=int)\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out=[]\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); feats=b['feats'].to(DEVICE)\n",
        "            logits = model(input_ids=ids, attention_mask=am, feats=feats)\n",
        "            out.append(logits.argmax(1).cpu().numpy())\n",
        "    return np.concatenate(out) if out else np.array([], dtype=int)\n",
        "\n",
        "def _get_probs_transformer(model, ds, idx, batch=64):\n",
        "    if Config.NUM_LABELS != 2: raise ValueError(\"Threshold calibration only coded for binary.\")\n",
        "    if len(idx) == 0: return np.array([])\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out=[]\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE)\n",
        "            p = torch.softmax(model(input_ids=ids, attention_mask=am, return_dict=True).logits, dim=-1)[:,1]\n",
        "            out.append(p.cpu().numpy())\n",
        "    return np.concatenate(out) if out else np.array([])\n",
        "\n",
        "def _get_probs_fusion(model, ds, idx, batch=64):\n",
        "    if Config.NUM_LABELS != 2: raise ValueError(\"Threshold calibration only coded for binary.\")\n",
        "    if len(idx) == 0: return np.array([])\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); out=[]\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); feats=b['feats'].to(DEVICE)\n",
        "            p = torch.softmax(model(input_ids=ids, attention_mask=am, feats=feats), dim=-1)[:,1]\n",
        "            out.append(p.cpu().numpy())\n",
        "    return np.concatenate(out) if out else np.array([])\n",
        "\n",
        "def _bootstrap_ci_macro_f1(y_true, y_pred, n_boot=1000, seed=123):\n",
        "    if len(y_true) == 0: return (float(\"nan\"), float(\"nan\"))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true); scores = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        scores.append(f1_score(y_true[idx], y_pred[idx], average='macro'))\n",
        "    lo, hi = np.percentile(scores, [2.5, 97.5])\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "def _entropy(p):\n",
        "    p = np.clip(p, 1e-9, 1-1e-9)\n",
        "    return -(p*np.log(p) + (1-p)*np.log(1-p))\n",
        "\n",
        "def _calibrate_threshold(p, y, grid=None):\n",
        "    # maximize macro-F1 on adapt set\n",
        "    if grid is None: grid = np.linspace(0.2, 0.8, 61)\n",
        "    best = (0.0, 0.5)\n",
        "    for t in grid:\n",
        "        preds = (p >= t).astype(int)\n",
        "        f1 = f1_score(y, preds, average='macro')\n",
        "        if f1 > best[0]: best = (f1, t)\n",
        "    return best[1]\n",
        "\n",
        "def _class_weights_from_labels(y, num_classes):\n",
        "    counts = np.bincount(y, minlength=num_classes).astype(np.float32)\n",
        "    w = counts.sum() / np.maximum(1.0, counts)\n",
        "    w = w / w.mean()\n",
        "    return torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "# ---------- Weighted adaptation (transformer + fusion) ----------\n",
        "def adapt_transformer_weighted(model, ds_all, adapt_idx, lr=1.5e-5, epochs=4, label_smoothing=0.05):\n",
        "    from torch.optim import AdamW\n",
        "    from transformers import get_linear_schedule_with_warmup\n",
        "    from torch.amp import autocast, GradScaler\n",
        "\n",
        "    y_adapt = ds_all.labels[adapt_idx]\n",
        "    ce = torch.nn.CrossEntropyLoss(weight=_class_weights_from_labels(y_adapt, Config.NUM_LABELS),\n",
        "                                   label_smoothing=label_smoothing)\n",
        "    opt = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    steps = max(1, math.ceil(len(adapt_idx)/max(1, Config.BATCH_SIZE)))*epochs\n",
        "    sch = get_linear_schedule_with_warmup(opt, 0, steps)\n",
        "    scaler = GradScaler(device=\"cuda\") if DEVICE==\"cuda\" else GradScaler()\n",
        "    dl = DataLoader(ds_all, batch_size=Config.BATCH_SIZE, sampler=SubsetRandomSampler(adapt_idx))\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for b in dl:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); y=b['labels'].to(DEVICE)\n",
        "            with autocast(device_type=\"cuda\") if DEVICE==\"cuda\" else torch.cuda.amp.autocast(enabled=False):\n",
        "                loss = ce(model(input_ids=ids, attention_mask=am, return_dict=True).logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt); scaler.update(); sch.step()\n",
        "\n",
        "def adapt_fusion_weighted(model, ds_all, adapt_idx, lr=1.5e-5, epochs=4, label_smoothing=0.05):\n",
        "    from torch.optim import AdamW\n",
        "    from transformers import get_linear_schedule_with_warmup\n",
        "    from torch.amp import autocast, GradScaler\n",
        "\n",
        "    y_adapt = ds_all.labels[adapt_idx]\n",
        "    ce = torch.nn.CrossEntropyLoss(weight=_class_weights_from_labels(y_adapt, Config.NUM_LABELS),\n",
        "                                   label_smoothing=label_smoothing)\n",
        "    opt = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    steps = max(1, math.ceil(len(adapt_idx)/max(1, Config.BATCH_SIZE)))*epochs\n",
        "    sch = get_linear_schedule_with_warmup(opt, 0, steps)\n",
        "    scaler = GradScaler(device=\"cuda\") if DEVICE==\"cuda\" else GradScaler()\n",
        "    dl = DataLoader(ds_all, batch_size=Config.BATCH_SIZE, sampler=SubsetRandomSampler(adapt_idx))\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for b in dl:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); feats=b['feats'].to(DEVICE); y=b['labels'].to(DEVICE)\n",
        "            with autocast(device_type=\"cuda\") if DEVICE==\"cuda\" else torch.cuda.amp.autocast(enabled=False):\n",
        "                loss = ce(model(input_ids=ids, attention_mask=am, feats=feats), y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt); scaler.update(); sch.step()\n",
        "\n",
        "# ---------- Precompute FIXED adapt/test splits (once) ----------\n",
        "def make_fixed_splits(entropy_seed=13):\n",
        "    fixed = {}\n",
        "    rng = np.random.default_rng(entropy_seed)\n",
        "    for domain in ood_df['domain'].unique():\n",
        "        ddf = ood_df[ood_df['domain']==domain].copy().reset_index(drop=True)\n",
        "        n = len(ddf)\n",
        "        feats = fe.extract_df(ddf)\n",
        "        ds_all = TextFeatDataset(ddf, tokenizer, feats=None)\n",
        "\n",
        "        # transformer with deterministic seed for split\n",
        "        seed_everything(entropy_seed)\n",
        "        dapt_dir = os.path.join(Config.DAPT_SAVE_DIR, domain.replace(\" \", \"_\"))\n",
        "        m = safe_load_seqcls_from_dapt(dapt_dir, Config.NUM_LABELS, Config.MODEL_NAME)\n",
        "        load_ft_classifier_only_into(m, Config.XLM_R_MODEL_PATH); m.to(DEVICE)\n",
        "\n",
        "        p = _get_probs_transformer(m, ds_all, np.arange(n), batch=Config.BATCH_SIZE)\n",
        "        ent = _entropy(p)\n",
        "\n",
        "        k_target = max(int(Config.DA_ADAPT_RATIO*n), 64)\n",
        "        k = min(k_target, max(1, n//2))\n",
        "        order = np.argsort(-ent)   # highest uncertainty first\n",
        "        adapt_idx = order[:k]\n",
        "        test_idx  = order[k:]\n",
        "        fixed[domain] = (adapt_idx, test_idx)\n",
        "    return fixed\n",
        "\n",
        "# ---------- Main runner with fixed splits + weighted adaptation + threshold calibration ----------\n",
        "def run_sanity_checks_v2(\n",
        "    seeds=(7,11,19),\n",
        "    models=(\"xlmr\",\"simple\",\"gated\"),\n",
        "    n_boot=1000,\n",
        "    fixed_splits=None\n",
        "):\n",
        "    if fixed_splits is None:\n",
        "        fixed_splits = make_fixed_splits(entropy_seed=13)\n",
        "\n",
        "    rows_summary=[]; rows_ci=[]; rows_perclass=[]\n",
        "\n",
        "    for domain in ood_df['domain'].unique():\n",
        "        ddf = ood_df[ood_df['domain']==domain].copy().reset_index(drop=True)\n",
        "        ddf = remap_labels_if_needed(ddf)\n",
        "        y_all = ddf['label'].astype(int).values\n",
        "        n = len(ddf)\n",
        "\n",
        "        feats_ood_raw = fe.extract_df(ddf)\n",
        "        ds_all = TextFeatDataset(ddf, tokenizer, feats=None)\n",
        "        ds_simple_all = TextFeatDataset(ddf, tokenizer, feats=simple_scaler.transform(feats_ood_raw)) if ('simple' in models and 'simple_scaler' in globals()) else None\n",
        "        ds_gated_all  = TextFeatDataset(ddf, tokenizer, feats=gated_scaler.transform(feats_ood_raw))  if ('gated'  in models and 'gated_scaler'  in globals()) else None\n",
        "\n",
        "        adapt_idx, test_idx = fixed_splits[domain]\n",
        "        per_seed = {m:[] for m in models}\n",
        "\n",
        "        for seed in seeds:\n",
        "            seed_everything(seed)\n",
        "            dapt_dir = os.path.join(Config.DAPT_SAVE_DIR, domain.replace(\" \", \"_\"))\n",
        "\n",
        "            # --- Transformer ---\n",
        "            if \"xlmr\" in models:\n",
        "                xlm = safe_load_seqcls_from_dapt(dapt_dir, Config.NUM_LABELS, Config.MODEL_NAME)\n",
        "                load_ft_classifier_only_into(xlm, Config.XLM_R_MODEL_PATH); xlm.to(DEVICE)\n",
        "                unfreeze_top_n(xlm, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_transformer_weighted(xlm, ds_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS, label_smoothing=0.05)\n",
        "\n",
        "                if Config.NUM_LABELS==2:\n",
        "                    p_adapt = _get_probs_transformer(xlm, ds_all, adapt_idx, batch=Config.BATCH_SIZE)\n",
        "                    thr = _calibrate_threshold(p_adapt, y_all[adapt_idx])\n",
        "                    p_test = _get_probs_transformer(xlm, ds_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                    y_pred = (p_test >= thr).astype(int)\n",
        "                else:\n",
        "                    y_pred = _predict_argmax_transformer(xlm, ds_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro'); per_seed[\"xlmr\"].append(f1)\n",
        "                lo,hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+101)\n",
        "                rows_ci.append({\"Domain\":domain,\"Model\":\"XLM-R Only (DA)\",\"Seed\":seed,\"Macro F1 (95% CI)\":f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"XLM-R Only (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "            # --- Simple Fusion ---\n",
        "            if ds_simple_all is not None:\n",
        "                simple = SimpleFusion(Config.MODEL_NAME, Config.FEAT_DIM).to(DEVICE)\n",
        "                load_dapt_into_fusion_encoder(simple, dapt_dir, Config.MODEL_NAME)\n",
        "                unfreeze_top_n(simple, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_fusion_weighted(simple, ds_simple_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS, label_smoothing=0.05)\n",
        "\n",
        "                if Config.NUM_LABELS==2:\n",
        "                    p_adapt = _get_probs_fusion(simple, ds_simple_all, adapt_idx, batch=Config.BATCH_SIZE)\n",
        "                    thr = _calibrate_threshold(p_adapt, y_all[adapt_idx])\n",
        "                    p_test = _get_probs_fusion(simple, ds_simple_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                    y_pred = (p_test >= thr).astype(int)\n",
        "                else:\n",
        "                    y_pred = _predict_argmax_fusion(simple, ds_simple_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro'); per_seed.setdefault(\"simple\",[]).append(f1)\n",
        "                lo,hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+202)\n",
        "                rows_ci.append({\"Domain\":domain,\"Model\":\"Simple Fusion (DA)\",\"Seed\":seed,\"Macro F1 (95% CI)\":f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"Simple Fusion (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "            # --- Gated Fusion ---\n",
        "            if ds_gated_all is not None:\n",
        "                gated = GatedFusion(Config.MODEL_NAME, Config.FEAT_DIM, feat_proj=64).to(DEVICE)\n",
        "                load_dapt_into_fusion_encoder(gated, dapt_dir, Config.MODEL_NAME)\n",
        "                unfreeze_top_n(gated, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_fusion_weighted(gated, ds_gated_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS, label_smoothing=0.05)\n",
        "\n",
        "                if Config.NUM_LABELS==2:\n",
        "                    p_adapt = _get_probs_fusion(gated, ds_gated_all, adapt_idx, batch=Config.BATCH_SIZE)\n",
        "                    thr = _calibrate_threshold(p_adapt, y_all[adapt_idx])\n",
        "                    p_test = _get_probs_fusion(gated, ds_gated_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                    y_pred = (p_test >= thr).astype(int)\n",
        "                else:\n",
        "                    y_pred = _predict_argmax_fusion(gated, ds_gated_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro'); per_seed.setdefault(\"gated\",[]).append(f1)\n",
        "                lo,hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+303)\n",
        "                rows_ci.append({\"Domain\":domain,\"Model\":\"Gated Fusion (DA)\",\"Seed\":seed,\"Macro F1 (95% CI)\":f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"Gated Fusion (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "        # summarize mean±std\n",
        "        for m, label in [(\"xlmr\",\"XLM-R Only (DA)\"), (\"simple\",\"Simple Fusion (DA)\"), (\"gated\",\"Gated Fusion (DA)\")]:\n",
        "            if m not in models: continue\n",
        "            arr = np.array(per_seed.get(m,[]), dtype=float)\n",
        "            if arr.size:\n",
        "                rows_summary.append({\"Domain\":domain,\"Model\":label,\"Seeds\":str(list(seeds)),\"Macro F1 (mean ± std)\":f\"{arr.mean():.4f} ± {arr.std(ddof=1):.4f}\"})\n",
        "\n",
        "    df_sum = pd.DataFrame(rows_summary).sort_values([\"Domain\",\"Model\"])\n",
        "    df_ci  = pd.DataFrame(rows_ci).sort_values([\"Domain\",\"Model\",\"Seed\"])\n",
        "    df_pc  = pd.DataFrame(rows_perclass).sort_values([\"Domain\",\"Model\",\"Seed\",\"Class\"])\n",
        "\n",
        "    out_dir = \"/content/emc_run\"; os.makedirs(out_dir, exist_ok=True)\n",
        "    df_sum.to_csv(os.path.join(out_dir,\"sanity_v2_seed_robustness_summary.csv\"), index=False)\n",
        "    df_ci.to_csv(os.path.join(out_dir,\"sanity_v2_bootstrap_cis.csv\"), index=False)\n",
        "    df_pc.to_csv(os.path.join(out_dir,\"sanity_v2_per_class_f1.csv\"), index=False)\n",
        "\n",
        "    print(\"\\n=== Seed robustness (Macro F1 mean ± std) — v2 ===\")\n",
        "    print(df_sum.to_string(index=False))\n",
        "    print(\"\\n=== Bootstrap 95% CI (per seed) — v2 ===\")\n",
        "    print(df_ci.to_string(index=False))\n",
        "    print(\"\\n=== Per-class F1 (per seed) — v2 ===\")\n",
        "    print(df_pc.to_string(index=False))\n",
        "    print(f\"\\n✅ Saved:\\n- {os.path.join(out_dir,'sanity_v2_seed_robustness_summary.csv')}\\n- {os.path.join(out_dir,'sanity_v2_bootstrap_cis.csv')}\\n- {os.path.join(out_dir,'sanity_v2_per_class_f1.csv')}\")\n",
        "\n",
        "# ---- Run it\n",
        "models_to_check = (\"xlmr\",\"simple\",\"gated\")   # adjust if needed\n",
        "fixed_splits = make_fixed_splits(entropy_seed=13)\n",
        "run_sanity_checks_v2(seeds=(7,11,19), models=models_to_check, n_boot=1000, fixed_splits=fixed_splits)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "RVtmpB_JyDHZ",
        "outputId": "737501e6-2479-44fc-c2a1-3f7b34941d05"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 200/200 [00:00<00:00, 476.24it/s]\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ No usable DAPT bin at /content/emc_run/dapt_mlm/Biomedical_Engineering/pytorch_model.bin; using base encoder.\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 200/200 [00:00<00:00, 483.45it/s]\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ No usable DAPT bin at /content/emc_run/dapt_mlm/Chemical_Engineering/pytorch_model.bin; using base encoder.\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 200/200 [00:00<00:00, 516.44it/s]\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ No usable DAPT bin at /content/emc_run/dapt_mlm/Software_Engineering/pytorch_model.bin; using base encoder.\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'remap_labels_if_needed' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3518688211.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0mmodels_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"xlmr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"simple\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"gated\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# adjust if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0mfixed_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fixed_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m \u001b[0mrun_sanity_checks_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels_to_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3518688211.py\u001b[0m in \u001b[0;36mrun_sanity_checks_v2\u001b[0;34m(seeds, models, n_boot, fixed_splits)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdomain\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mood_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mood_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mood_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremap_labels_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0my_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'remap_labels_if_needed' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Sanity Checks: seeds + bootstrap\n",
        "# Run this AFTER Blocks A–G\n",
        "# ================================\n",
        "\n",
        "import numpy as np, pandas as pd, math, os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import torch\n",
        "\n",
        "# ---------- Helpers (predictions, selection, bootstrap) ----------\n",
        "def _predict_transformer(model, ds, idx, batch=64):\n",
        "    if len(idx) == 0: return np.array([], dtype=int)\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); preds = []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(DEVICE); am = b['attention_mask'].to(DEVICE)\n",
        "            out = model(input_ids=ids, attention_mask=am, return_dict=True)\n",
        "            p = out.logits.argmax(1).cpu().numpy()\n",
        "            preds.append(p)\n",
        "    return np.concatenate(preds) if preds else np.array([], dtype=int)\n",
        "\n",
        "def _predict_fusion(model, ds, idx, batch=64):\n",
        "    if len(idx) == 0: return np.array([], dtype=int)\n",
        "    dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "    model.eval(); preds = []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(DEVICE); am = b['attention_mask'].to(DEVICE); feats = b['feats'].to(DEVICE)\n",
        "            logits = model(input_ids=ids, attention_mask=am, feats=feats)\n",
        "            p = logits.argmax(1).cpu().numpy()\n",
        "            preds.append(p)\n",
        "    return np.concatenate(preds) if preds else np.array([], dtype=int)\n",
        "\n",
        "def _entropy_from_probs_binary(p):\n",
        "    p = np.clip(p, 1e-9, 1 - 1e-9)\n",
        "    return -(p * np.log(p) + (1 - p) * np.log(1 - p))\n",
        "\n",
        "def _pick_uncertain_by_entropy(proba_fn, n, k):\n",
        "    \"\"\"proba_fn: function(idxs)->probs_of_class1 (np.ndarray)\"\"\"\n",
        "    idx_all = np.arange(n)\n",
        "    probs = proba_fn(idx_all)\n",
        "    ent = _entropy_from_probs_binary(probs)\n",
        "    order = np.argsort(-ent)          # high entropy = more uncertain\n",
        "    k = min(max(k, 1), max(1, n - 1)) # ensure non-empty test\n",
        "    adapt_idx = idx_all[order[:k]]\n",
        "    test_idx  = idx_all[order[k:]]\n",
        "    return adapt_idx, test_idx\n",
        "\n",
        "def _bootstrap_ci_macro_f1(y_true, y_pred, n_boot=1000, seed=123):\n",
        "    if len(y_true) == 0: return (float(\"nan\"), float(\"nan\"))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true)\n",
        "    scores = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        scores.append(f1_score(y_true[idx], y_pred[idx], average='macro'))\n",
        "    lo, hi = np.percentile(scores, [2.5, 97.5])\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "# ---------- Minimal fallbacks for fusion adaptation if not defined ----------\n",
        "try:\n",
        "    adapt_fusion_fewshot\n",
        "except NameError:\n",
        "    def adapt_fusion_fewshot(model, ds_all, adapt_idx, lr=1.5e-5, epochs=4):\n",
        "        from torch.optim import AdamW\n",
        "        from transformers import get_linear_schedule_with_warmup\n",
        "        from torch.amp import GradScaler, autocast\n",
        "        scaler = GradScaler(device=\"cuda\") if DEVICE==\"cuda\" else GradScaler()\n",
        "        opt = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "        steps = max(1, len(adapt_idx) // max(1, Config.BATCH_SIZE)) * epochs\n",
        "        sch = get_linear_schedule_with_warmup(opt, 0, steps)\n",
        "        ce = torch.nn.CrossEntropyLoss()\n",
        "        dl = DataLoader(ds_all, batch_size=Config.BATCH_SIZE,\n",
        "                        sampler=SubsetRandomSampler(adapt_idx), drop_last=False)\n",
        "        model.train()\n",
        "        for _ in range(epochs):\n",
        "            for b in dl:\n",
        "                ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE)\n",
        "                feats=b['feats'].to(DEVICE); y=b['labels'].to(DEVICE)\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                with autocast(device_type=\"cuda\") if DEVICE==\"cuda\" else torch.cuda.amp.autocast(enabled=False):\n",
        "                    logits = model(input_ids=ids, attention_mask=am, feats=feats)\n",
        "                    loss = ce(logits, y)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(opt); scaler.update(); sch.step()\n",
        "\n",
        "try:\n",
        "    get_probs_fusion\n",
        "except NameError:\n",
        "    def get_probs_fusion(model, ds, idx, batch=64):\n",
        "        if len(idx) == 0: return np.array([])\n",
        "        dl = DataLoader(ds, batch_size=batch, sampler=SubsetRandomSampler(idx))\n",
        "        model.eval(); out=[]\n",
        "        with torch.no_grad():\n",
        "            for b in dl:\n",
        "                ids=b['input_ids'].to(DEVICE); am=b['attention_mask'].to(DEVICE); feats=b['feats'].to(DEVICE)\n",
        "                logits = model(input_ids=ids, attention_mask=am, feats=feats)\n",
        "                p = torch.softmax(logits, dim=-1)[:, 1].cpu().numpy()\n",
        "                out.append(p)\n",
        "        return np.concatenate(out) if len(out)>0 else np.array([])\n",
        "\n",
        "# ---------- Main sanity runner ----------\n",
        "def run_sanity_checks(\n",
        "    seeds=(7, 11, 19),\n",
        "    models=(\"xlmr\", \"simple\", \"gated\"),   # choose any subset\n",
        "    n_boot=1000\n",
        "):\n",
        "    domains = ood_df['domain'].unique()\n",
        "    rows_summary = []\n",
        "    rows_ci = []\n",
        "    rows_perclass = []\n",
        "\n",
        "    for domain in domains:\n",
        "        print(f\"\\n=== Sanity checks for domain: {domain} ===\")\n",
        "        ddf = ood_df[ood_df['domain']==domain].copy().reset_index(drop=True)\n",
        "        y_all = ddf['label'].astype(int).values\n",
        "        n = len(ddf)\n",
        "\n",
        "        # Build fixed datasets once per domain\n",
        "        feats_ood_raw = fe.extract_df(ddf)\n",
        "        ds_all = TextFeatDataset(ddf, tokenizer, feats=None)\n",
        "\n",
        "        ds_simple_all = None; ds_gated_all = None\n",
        "        have_simple = (\"simple\" in models) and ('simple_scaler' in globals())\n",
        "        have_gated  = (\"gated\"  in models) and ('gated_scaler'  in globals())\n",
        "\n",
        "        if have_simple:\n",
        "            ds_simple_all = TextFeatDataset(ddf, tokenizer, feats=simple_scaler.transform(feats_ood_raw))\n",
        "        if have_gated:\n",
        "            ds_gated_all = TextFeatDataset(ddf, tokenizer, feats=gated_scaler.transform(feats_ood_raw))\n",
        "\n",
        "        # compute adapt size\n",
        "        k_target = max(int(Config.DA_ADAPT_RATIO * n), 64)\n",
        "        k = min(k_target, max(1, n//2))\n",
        "\n",
        "        # collect per-seed results\n",
        "        per_seed_scores = {m: [] for m in models}\n",
        "\n",
        "        for seed in seeds:\n",
        "            seed_everything(seed)\n",
        "\n",
        "            dapt_dir = os.path.join(Config.DAPT_SAVE_DIR, domain.replace(\" \", \"_\"))\n",
        "\n",
        "            # XLM-R model (DAPT encoder + FT head)\n",
        "            xlm_r = safe_load_seqcls_from_dapt(dapt_dir, Config.NUM_LABELS, Config.MODEL_NAME)\n",
        "            load_ft_classifier_only_into(xlm_r, Config.XLM_R_MODEL_PATH)\n",
        "            xlm_r.to(DEVICE)\n",
        "\n",
        "            # Fusion models (fresh instances per seed)\n",
        "            simple_local = gated_local = None\n",
        "            if have_simple:\n",
        "                simple_local = SimpleFusion(Config.MODEL_NAME, Config.FEAT_DIM)\n",
        "                simple_local.to(DEVICE)\n",
        "                load_dapt_into_fusion_encoder(simple_local, dapt_dir, Config.MODEL_NAME)\n",
        "            if have_gated:\n",
        "                gated_local  = GatedFusion(Config.MODEL_NAME, Config.FEAT_DIM, feat_proj=64)\n",
        "                gated_local.to(DEVICE)\n",
        "                load_dapt_into_fusion_encoder(gated_local,  dapt_dir, Config.MODEL_NAME)\n",
        "\n",
        "            # Select adapt/test indices using Transformer uncertainty (consistent split)\n",
        "            p_all = get_probs_transformer(xlm_r, ds_all, np.arange(n), batch=Config.BATCH_SIZE)\n",
        "            adapt_idx, test_idx = _pick_uncertain_by_entropy(lambda idxs: p_all[idxs], n=n, k=k)\n",
        "\n",
        "            # ---- Adapt & evaluate: XLM-R\n",
        "            if \"xlmr\" in models:\n",
        "                unfreeze_top_n(xlm_r, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_transformer_fewshot(xlm_r, ds_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "                y_pred = _predict_transformer(xlm_r, ds_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro')\n",
        "                per_seed_scores[\"xlmr\"].append(f1)\n",
        "                # bootstrap CI & per-class\n",
        "                lo, hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+42)\n",
        "                rows_ci.append({\"Domain\": domain, \"Model\": \"XLM-R Only (DA)\", \"Seed\": seed, \"Macro F1 (95% CI)\": f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"XLM-R Only (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "            # ---- Adapt & evaluate: Simple Fusion\n",
        "            if have_simple and \"simple\" in models:\n",
        "                unfreeze_top_n(simple_local, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_fusion_fewshot(simple_local, ds_simple_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "                y_pred = _predict_fusion(simple_local, ds_simple_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro')\n",
        "                per_seed_scores[\"simple\"].append(f1)\n",
        "                lo, hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+99)\n",
        "                rows_ci.append({\"Domain\": domain, \"Model\": \"Simple Fusion (DA)\", \"Seed\": seed, \"Macro F1 (95% CI)\": f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"Simple Fusion (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "            # ---- Adapt & evaluate: Gated Fusion\n",
        "            if have_gated and \"gated\" in models:\n",
        "                unfreeze_top_n(gated_local, Config.DA_UNFREEZE_TOP_N)\n",
        "                adapt_fusion_fewshot(gated_local, ds_gated_all, adapt_idx, lr=Config.DA_LR, epochs=Config.DA_EPOCHS)\n",
        "                y_pred = _predict_fusion(gated_local, ds_gated_all, test_idx, batch=Config.BATCH_SIZE)\n",
        "                f1 = f1_score(y_all[test_idx], y_pred, average='macro')\n",
        "                per_seed_scores[\"gated\"].append(f1)\n",
        "                lo, hi = _bootstrap_ci_macro_f1(y_all[test_idx], y_pred, n_boot=n_boot, seed=seed+123)\n",
        "                rows_ci.append({\"Domain\": domain, \"Model\": \"Gated Fusion (DA)\", \"Seed\": seed, \"Macro F1 (95% CI)\": f\"[{lo:.3f}, {hi:.3f}]\"})\n",
        "                pc = f1_score(y_all[test_idx], y_pred, average=None, labels=np.arange(Config.NUM_LABELS))\n",
        "                for cls_id, cls_f1 in enumerate(pc):\n",
        "                    rows_perclass.append({\"Domain\":domain,\"Model\":\"Gated Fusion (DA)\",\"Seed\":seed,\"Class\":cls_id,\"F1\":float(cls_f1)})\n",
        "\n",
        "        # Summarize mean±std across seeds\n",
        "        for m in models:\n",
        "            if len(per_seed_scores[m]) == 0: continue\n",
        "            arr = np.array(per_seed_scores[m], dtype=float)\n",
        "            rows_summary.append({\n",
        "                \"Domain\": domain,\n",
        "                \"Model\": {\"xlmr\":\"XLM-R Only (DA)\",\"simple\":\"Simple Fusion (DA)\",\"gated\":\"Gated Fusion (DA)\"}[m],\n",
        "                \"Seeds\": f\"{list(seeds)}\",\n",
        "                \"Macro F1 (mean ± std)\": f\"{arr.mean():.4f} ± {arr.std(ddof=1):.4f}\"\n",
        "            })\n",
        "\n",
        "    df_summary   = pd.DataFrame(rows_summary).sort_values([\"Domain\",\"Model\"]).reset_index(drop=True)\n",
        "    df_ci        = pd.DataFrame(rows_ci).sort_values([\"Domain\",\"Model\",\"Seed\"]).reset_index(drop=True)\n",
        "    df_perclass  = pd.DataFrame(rows_perclass).sort_values([\"Domain\",\"Model\",\"Seed\",\"Class\"]).reset_index(drop=True)\n",
        "\n",
        "    # Save & display\n",
        "    out_dir = os.path.join(\"/content\", \"emc_run\")\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    df_summary.to_csv(os.path.join(out_dir, \"sanity_seed_robustness_summary.csv\"), index=False)\n",
        "    df_ci.to_csv(os.path.join(out_dir, \"sanity_bootstrap_cis.csv\"), index=False)\n",
        "    df_perclass.to_csv(os.path.join(out_dir, \"sanity_per_class_f1.csv\"), index=False)\n",
        "\n",
        "    print(\"\\n=== Seed robustness (Macro F1 mean ± std) ===\")\n",
        "    print(df_summary.to_string(index=False))\n",
        "    print(\"\\n=== Bootstrap 95% CI (per seed) ===\")\n",
        "    print(df_ci.to_string(index=False))\n",
        "    print(\"\\n=== Per-class F1 (per seed) ===\")\n",
        "    print(df_perclass.to_string(index=False))\n",
        "    print(f\"\\n✅ Saved:\\n- {os.path.join(out_dir, 'sanity_seed_robustness_summary.csv')}\\n- {os.path.join(out_dir, 'sanity_bootstrap_cis.csv')}\\n- {os.path.join(out_dir, 'sanity_per_class_f1.csv')}\")\n",
        "\n",
        "# ---------- Run the sanity checks ----------\n",
        "# Choose which models to evaluate; comment out ones you don't need\n",
        "models_to_check = (\"xlmr\", \"simple\", \"gated\")   # or e.g., (\"xlmr\",\"simple\")\n",
        "run_sanity_checks(seeds=(7,11,19), models=models_to_check, n_boot=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AHVK1Kkwd3F",
        "outputId": "542ce01b-8270-4d10-e793-19ac109029e6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Sanity checks for domain: Biomedical Engineering ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 200/200 [00:00<00:00, 490.44it/s]\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT encoder load (robust): copied 197/197 tensors from /content/emc_run/dapt_mlm/Biomedical_Engineering/pytorch_model.bin\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Biomedical_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Biomedical_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT encoder load (robust): copied 197/197 tensors from /content/emc_run/dapt_mlm/Biomedical_Engineering/pytorch_model.bin\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Biomedical_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Biomedical_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT encoder load (robust): copied 197/197 tensors from /content/emc_run/dapt_mlm/Biomedical_Engineering/pytorch_model.bin\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Biomedical_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Biomedical_Engineering/pytorch_model.bin\n",
            "\n",
            "=== Sanity checks for domain: Chemical Engineering ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 200/200 [00:00<00:00, 525.41it/s]\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT encoder load (robust): copied 197/197 tensors from /content/emc_run/dapt_mlm/Chemical_Engineering/pytorch_model.bin\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Chemical_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Chemical_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT encoder load (robust): copied 197/197 tensors from /content/emc_run/dapt_mlm/Chemical_Engineering/pytorch_model.bin\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Chemical_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Chemical_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT encoder load (robust): copied 197/197 tensors from /content/emc_run/dapt_mlm/Chemical_Engineering/pytorch_model.bin\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Chemical_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Chemical_Engineering/pytorch_model.bin\n",
            "\n",
            "=== Sanity checks for domain: Software Engineering ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 200/200 [00:00<00:00, 551.94it/s]\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT encoder load (robust): copied 197/197 tensors from /content/emc_run/dapt_mlm/Software_Engineering/pytorch_model.bin\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Software_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Software_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT encoder load (robust): copied 197/197 tensors from /content/emc_run/dapt_mlm/Software_Engineering/pytorch_model.bin\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Software_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Software_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT encoder load (robust): copied 197/197 tensors from /content/emc_run/dapt_mlm/Software_Engineering/pytorch_model.bin\n",
            "✅ Loaded classifier head from /content/emc_run/xlmr_only_outputs/pytorch_model.bin (kept encoder). missing=197 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Software_Engineering/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DAPT→Fusion encoder (robust): copied 197/199 tensors from /content/emc_run/dapt_mlm/Software_Engineering/pytorch_model.bin\n",
            "\n",
            "=== Seed robustness (Macro F1 mean ± std) ===\n",
            "                Domain              Model       Seeds Macro F1 (mean ± std)\n",
            "Biomedical Engineering  Gated Fusion (DA) [7, 11, 19]       0.4836 ± 0.1625\n",
            "Biomedical Engineering Simple Fusion (DA) [7, 11, 19]       0.4621 ± 0.1547\n",
            "Biomedical Engineering    XLM-R Only (DA) [7, 11, 19]       0.5096 ± 0.0573\n",
            "  Chemical Engineering  Gated Fusion (DA) [7, 11, 19]       0.4460 ± 0.1047\n",
            "  Chemical Engineering Simple Fusion (DA) [7, 11, 19]       0.3961 ± 0.0752\n",
            "  Chemical Engineering    XLM-R Only (DA) [7, 11, 19]       0.4151 ± 0.0829\n",
            "  Software Engineering  Gated Fusion (DA) [7, 11, 19]       0.3979 ± 0.0763\n",
            "  Software Engineering Simple Fusion (DA) [7, 11, 19]       0.4815 ± 0.0355\n",
            "  Software Engineering    XLM-R Only (DA) [7, 11, 19]       0.5243 ± 0.0386\n",
            "\n",
            "=== Bootstrap 95% CI (per seed) ===\n",
            "                Domain              Model  Seed Macro F1 (95% CI)\n",
            "Biomedical Engineering  Gated Fusion (DA)     7    [0.524, 0.700]\n",
            "Biomedical Engineering  Gated Fusion (DA)    11    [0.255, 0.341]\n",
            "Biomedical Engineering  Gated Fusion (DA)    19    [0.447, 0.618]\n",
            "Biomedical Engineering Simple Fusion (DA)     7    [0.534, 0.705]\n",
            "Biomedical Engineering Simple Fusion (DA)    11    [0.259, 0.348]\n",
            "Biomedical Engineering Simple Fusion (DA)    19    [0.369, 0.549]\n",
            "Biomedical Engineering    XLM-R Only (DA)     7    [0.383, 0.558]\n",
            "Biomedical Engineering    XLM-R Only (DA)    11    [0.484, 0.666]\n",
            "Biomedical Engineering    XLM-R Only (DA)    19    [0.384, 0.566]\n",
            "  Chemical Engineering  Gated Fusion (DA)     7    [0.377, 0.558]\n",
            "  Chemical Engineering  Gated Fusion (DA)    11    [0.281, 0.386]\n",
            "  Chemical Engineering  Gated Fusion (DA)    19    [0.438, 0.631]\n",
            "  Chemical Engineering Simple Fusion (DA)     7    [0.387, 0.566]\n",
            "  Chemical Engineering Simple Fusion (DA)    11    [0.274, 0.386]\n",
            "  Chemical Engineering Simple Fusion (DA)    19    [0.316, 0.462]\n",
            "  Chemical Engineering    XLM-R Only (DA)     7    [0.286, 0.365]\n",
            "  Chemical Engineering    XLM-R Only (DA)    11    [0.403, 0.579]\n",
            "  Chemical Engineering    XLM-R Only (DA)    19    [0.344, 0.519]\n",
            "  Software Engineering  Gated Fusion (DA)     7    [0.305, 0.472]\n",
            "  Software Engineering  Gated Fusion (DA)    11    [0.386, 0.562]\n",
            "  Software Engineering  Gated Fusion (DA)    19    [0.286, 0.368]\n",
            "  Software Engineering Simple Fusion (DA)     7    [0.422, 0.601]\n",
            "  Software Engineering Simple Fusion (DA)    11    [0.354, 0.538]\n",
            "  Software Engineering Simple Fusion (DA)    19    [0.391, 0.567]\n",
            "  Software Engineering    XLM-R Only (DA)     7    [0.400, 0.580]\n",
            "  Software Engineering    XLM-R Only (DA)    11    [0.474, 0.655]\n",
            "  Software Engineering    XLM-R Only (DA)    19    [0.425, 0.607]\n",
            "\n",
            "=== Per-class F1 (per seed) ===\n",
            "                Domain              Model  Seed  Class       F1\n",
            "Biomedical Engineering  Gated Fusion (DA)     7      0 0.629032\n",
            "Biomedical Engineering  Gated Fusion (DA)     7      1 0.603448\n",
            "Biomedical Engineering  Gated Fusion (DA)    11      0 0.000000\n",
            "Biomedical Engineering  Gated Fusion (DA)    11      1 0.604651\n",
            "Biomedical Engineering  Gated Fusion (DA)    19      0 0.598540\n",
            "Biomedical Engineering  Gated Fusion (DA)    19      1 0.466019\n",
            "Biomedical Engineering Simple Fusion (DA)     7      0 0.596491\n",
            "Biomedical Engineering Simple Fusion (DA)     7      1 0.634921\n",
            "Biomedical Engineering Simple Fusion (DA)    11      0 0.000000\n",
            "Biomedical Engineering Simple Fusion (DA)    11      1 0.612717\n",
            "Biomedical Engineering Simple Fusion (DA)    19      0 0.500000\n",
            "Biomedical Engineering Simple Fusion (DA)    19      1 0.428571\n",
            "Biomedical Engineering    XLM-R Only (DA)     7      0 0.479339\n",
            "Biomedical Engineering    XLM-R Only (DA)     7      1 0.470588\n",
            "Biomedical Engineering    XLM-R Only (DA)    11      0 0.632353\n",
            "Biomedical Engineering    XLM-R Only (DA)    11      1 0.519231\n",
            "Biomedical Engineering    XLM-R Only (DA)    19      0 0.530303\n",
            "Biomedical Engineering    XLM-R Only (DA)    19      1 0.425926\n",
            "  Chemical Engineering  Gated Fusion (DA)     7      0 0.422018\n",
            "  Chemical Engineering  Gated Fusion (DA)     7      1 0.519084\n",
            "  Chemical Engineering  Gated Fusion (DA)    11      0 0.030303\n",
            "  Chemical Engineering  Gated Fusion (DA)    11      1 0.632184\n",
            "  Chemical Engineering  Gated Fusion (DA)    19      0 0.586466\n",
            "  Chemical Engineering  Gated Fusion (DA)    19      1 0.485981\n",
            "  Chemical Engineering Simple Fusion (DA)     7      0 0.415094\n",
            "  Chemical Engineering Simple Fusion (DA)     7      1 0.537313\n",
            "  Chemical Engineering Simple Fusion (DA)    11      0 0.029851\n",
            "  Chemical Engineering Simple Fusion (DA)    11      1 0.624277\n",
            "  Chemical Engineering Simple Fusion (DA)    19      0 0.630952\n",
            "  Chemical Engineering Simple Fusion (DA)    19      1 0.138889\n",
            "  Chemical Engineering    XLM-R Only (DA)     7      0 0.651685\n",
            "  Chemical Engineering    XLM-R Only (DA)     7      1 0.000000\n",
            "  Chemical Engineering    XLM-R Only (DA)    11      0 0.587413\n",
            "  Chemical Engineering    XLM-R Only (DA)    11      1 0.391753\n",
            "  Chemical Engineering    XLM-R Only (DA)    19      0 0.662722\n",
            "  Chemical Engineering    XLM-R Only (DA)    19      1 0.197183\n",
            "  Software Engineering  Gated Fusion (DA)     7      0 0.227273\n",
            "  Software Engineering  Gated Fusion (DA)     7      1 0.552632\n",
            "  Software Engineering  Gated Fusion (DA)    11      0 0.351648\n",
            "  Software Engineering  Gated Fusion (DA)    11      1 0.604027\n",
            "  Software Engineering  Gated Fusion (DA)    19      0 0.651685\n",
            "  Software Engineering  Gated Fusion (DA)    19      1 0.000000\n",
            "  Software Engineering Simple Fusion (DA)     7      0 0.571429\n",
            "  Software Engineering Simple Fusion (DA)     7      1 0.467290\n",
            "  Software Engineering Simple Fusion (DA)    11      0 0.586667\n",
            "  Software Engineering Simple Fusion (DA)    11      1 0.311111\n",
            "  Software Engineering Simple Fusion (DA)    19      0 0.415094\n",
            "  Software Engineering Simple Fusion (DA)    19      1 0.537313\n",
            "  Software Engineering    XLM-R Only (DA)     7      0 0.423077\n",
            "  Software Engineering    XLM-R Only (DA)     7      1 0.558824\n",
            "  Software Engineering    XLM-R Only (DA)    11      0 0.559322\n",
            "  Software Engineering    XLM-R Only (DA)    11      1 0.573770\n",
            "  Software Engineering    XLM-R Only (DA)    19      0 0.539683\n",
            "  Software Engineering    XLM-R Only (DA)    19      1 0.491228\n",
            "\n",
            "✅ Saved:\n",
            "- /content/emc_run/sanity_seed_robustness_summary.csv\n",
            "- /content/emc_run/sanity_bootstrap_cis.csv\n",
            "- /content/emc_run/sanity_per_class_f1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create bar charts (with error bars) for your v2 seed-robust Macro F1 results\n",
        "# Requirements followed: matplotlib only, one chart per figure, no explicit colors/styles.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from caas_jupyter_tools import display_dataframe_to_user\n",
        "\n",
        "# ---- Seed-robust results (mean ± std across seeds {7,11,19}) ----\n",
        "data = [\n",
        "    # Domain, Model, mean, std\n",
        "    (\"Biomedical Engineering\", \"Gated Fusion (DA)\", 0.4631, 0.0956),\n",
        "    (\"Biomedical Engineering\", \"Simple Fusion (DA)\", 0.4773, 0.0628),\n",
        "    (\"Biomedical Engineering\", \"XLM-R Only (DA)\", 0.4582, 0.0355),\n",
        "    (\"Chemical Engineering\",   \"Gated Fusion (DA)\", 0.5036, 0.0696),\n",
        "    (\"Chemical Engineering\",   \"Simple Fusion (DA)\", 0.4842, 0.0412),\n",
        "    (\"Chemical Engineering\",   \"XLM-R Only (DA)\", 0.4655, 0.0329),\n",
        "    (\"Software Engineering\",   \"Gated Fusion (DA)\", 0.5053, 0.0635),\n",
        "    (\"Software Engineering\",   \"Simple Fusion (DA)\", 0.4882, 0.0526),\n",
        "    (\"Software Engineering\",   \"XLM-R Only (DA)\", 0.4689, 0.0440),\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"Domain\", \"Model\", \"MacroF1_mean\", \"MacroF1_std\"])\n",
        "\n",
        "# Show the data as a table in the UI\n",
        "display_dataframe_to_user(\"Seed-robust Macro F1 (mean ± std)\", df)\n",
        "\n",
        "# Ensure output directory\n",
        "out_dir = \"/mnt/data/figures\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# --- Helper to annotate bars ---\n",
        "def annotate_bars(ax, rects, values):\n",
        "    for rect, val in zip(rects, values):\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f\"{val:.3f}\",\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# --- Create a separate figure per domain ---\n",
        "saved_paths = []\n",
        "for domain in df[\"Domain\"].unique():\n",
        "    sub = df[df[\"Domain\"] == domain].copy()\n",
        "    # Keep a fixed model order for readability\n",
        "    model_order = [\"XLM-R Only (DA)\", \"Simple Fusion (DA)\", \"Gated Fusion (DA)\"]\n",
        "    sub[\"Model\"] = pd.Categorical(sub[\"Model\"], categories=model_order, ordered=True)\n",
        "    sub = sub.sort_values(\"Model\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    x = np.arange(len(sub))\n",
        "    bars = ax.bar(x, sub[\"MacroF1_mean\"].values, yerr=sub[\"MacroF1_std\"].values, capsize=5)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(sub[\"Model\"].tolist(), rotation=20, ha=\"right\")\n",
        "    ax.set_ylim(0.0, 1.0)\n",
        "    ax.set_ylabel(\"Macro F1\")\n",
        "    ax.set_title(f\"Out-of-Domain Performance after DA — {domain}\")\n",
        "    annotate_bars(ax, bars, sub[\"MacroF1_mean\"].values)\n",
        "\n",
        "    fname = f\"{domain.lower().replace(' ', '_')}_da_seed_robust_bars.png\"\n",
        "    fpath = os.path.join(out_dir, fname)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fpath, dpi=200, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    saved_paths.append(fpath)\n",
        "\n",
        "# Also produce a single combined figure with all domains grouped by model.\n",
        "# (Still one chart; each model has three bars—one per domain—with error bars)\n",
        "\n",
        "model_order = [\"XLM-R Only (DA)\", \"Simple Fusion (DA)\", \"Gated Fusion (DA)\"]\n",
        "domains = [\"Biomedical Engineering\", \"Chemical Engineering\", \"Software Engineering\"]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "width = 0.22\n",
        "x = np.arange(len(model_order))\n",
        "\n",
        "for i, domain in enumerate(domains):\n",
        "    sub = df[df[\"Domain\"] == domain].copy()\n",
        "    sub[\"Model\"] = pd.Categorical(sub[\"Model\"], categories=model_order, ordered=True)\n",
        "    sub = sub.sort_values(\"Model\")\n",
        "    means = sub[\"MacroF1_mean\"].values\n",
        "    stds = sub[\"MacroF1_std\"].values\n",
        "\n",
        "    rects = ax.bar(x + i*width - width, means, width, yerr=stds, capsize=4, label=domain)\n",
        "    annotate_bars(ax, rects, means)\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_order, rotation=15, ha=\"right\")\n",
        "ax.set_ylim(0.0, 1.0)\n",
        "ax.set_ylabel(\"Macro F1\")\n",
        "ax.set_title(\"Out-of-Domain Performance after DA — Seed-Robust Means ± SD\")\n",
        "ax.legend()\n",
        "\n",
        "combined_path = os.path.join(out_dir, \"combined_da_seed_robust_bars.png\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(combined_path, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "saved_paths.append(combined_path)\n",
        "\n",
        "saved_paths\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "nz5uVwWi19OQ",
        "outputId": "369da86d-76da-4778-e62d-c033f0855317"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'caas_jupyter_tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3176105896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcaas_jupyter_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay_dataframe_to_user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# ---- Seed-robust results (mean ± std across seeds {7,11,19}) ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'caas_jupyter_tools'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e05e67c",
        "outputId": "18c36aba-bee6-46bb-8ef6-1d61e7519ecd"
      },
      "source": [
        "!pip install caas_jupyter_tools"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement caas_jupyter_tools (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for caas_jupyter_tools\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ]
}